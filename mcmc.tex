% to-do
% -----
% - write zeroth draft
% - write first draft
% - send to friendlies for comments
%   - Lang
%   - Rix
%   - Bovy
%   - Brewer
% - post on arXiv

% style notes
% -----------
% - [LaTeX] newline after every full stop for proper git diffing
% - [LaTeX] eqnarray not equation
% - pdf not PDF
% - is it ``a MCMC sampling'' or ``an MCMC sampling''?

\documentclass[12pt,twoside,pdftex]{article}
\usepackage{styles/dar_endnotes}

\newcommand{\this}{Using Markov Chain Monte Carlo}
\include{styles/dar}

\newcommand{\data}{D}
\newcommand{\pars}{\theta}
\newcommand{\best}{{\mathrm{(best)}}}
\newcommand{\better}{{\mathrm{(better)}}}

\begin{document}

\thispagestyle{plain}\raggedbottom
\section*{Data analysis recipes:\\ \this\footnotemark}

\footnotetext{%
    The notes begin on page~\pageref{note:first}, including the
    license\note{\label{note:first}
        Copyright 2013 by the authors. This work is licensed under a
        \href{http://creativecommons.org/licenses/by-nc-nd/3.0/deed.en\_US}{%
            Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported
            License}.}
    and the acknowledgements\note{%
        We would like to thank
          Jo Bovy (IAS),
          Brendon Brewer (Auckland),
          Jonathan Goodman (NYU),
          Fengji Hou (NYU), and
          Dustin Lang (CMU)
        for valuable advice and comments.}.
}

\noindent
Dan~Foreman-Mackey\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics,
       New York University}\\[1ex]
David~W.~Hogg\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics,
       New York University}\\
\affil{Max-Planck-Institut f\"ur Astronomie, Heidelberg}

\begin{abstract}
Markov Chain Monte Carlo (MCMC) methods for sampling probability
distribution functions, plus abundant computational resources,
have transformed the sciences.
Here we give a fast overview of basic MCMC methods and then turn to
practical advice for their use in real inference problems.
We give advice on method choice, tuning for performance,
initialization and burn-in, judging convergence, and use of the chain
output to produce parameter estimates with associated uncertainties.
[Insert some generally useful point here!]
\end{abstract}

\section{When do you need MCMC?}

Markov Chain Monte Carlo (MCMC) methods are methods for sampling
probability distribution functions or probability density functions (pdfs).
These pdfs may be either probability mass functions on a discrete
space, or probability densities on a continuous space, though we will
concentrate on the latter in this \documentname.
MCMC methods don't require that you have a full analytic description of the
properly normalized pdf for sampling to proceed; they only require
that you be able to compute ratios of the pdf at pairs of locations.
This makes MCMC methods ideal for sampling \emph{posterior
  pdfs} in probabilistic inferences:

In a probabilistic inference, the posterior pdf $p(\pars\given\data)$,
or pdf for the parameters $\pars$ given the data $\data$, is
constructed from the likelihood $p(\data\given\pars)$, or pdf for the
data given the parameters, and the prior pdf $p(\pars)$ for the
parameters by what's often known as ``Bayes rule'',
\begin{eqnarray}
p(\pars\given\data) &=& \frac{1}{Z}\,p(\data\given\pars)\,p(\pars)
\quad .
\end{eqnarray}
In these contexts, the constant $Z$, sometimes written as
$p(\data)$, is known by the names ``evidence'', ``marginal likelihood''
the ``Bayes integral'' and ``prior predictive probability'', and is usually
\emph{extremely hard to
  calculate}.\note{Say words about why we call it $Z$ not $p(\data)$
  and why it is extremely hard to calculate.}
That is, you often know the function $p(\pars\given\data)$ up to a
constant factor; you can compute ratios of the pdf at pairs of points,
but not the precise value at any individual point.

In addition to this normalization-insensitive property of MCMC, in its
simplest forms it can be run without computing any derivatives or
integrals of the function, and (as we will show below) in its simplest
forms it is \emph{extremely easy to implement}.
For all these reasons, MCMC is ideal for sampling posterior pdfs in
the real situations in which scientists find themselves.

Say you are in this situation.
You have a huge blob of data $\data$ (think of this as a vector or
list or heterogeneous collection of observations\note{In general in
  this \documentname\ we will treat any variable as being capable of
  containing not just a value but some huge arbitrarily structured set
  of values in matrix, vector, or more complex form.}).
You also have a model sophisticated enough---a probabilistic,
generative model, if you will\note{For our definition of a
  \emph{model}, see [SOME OTHER NOTE] in this series.}---that, given a
setting of a huge blob of parameters\note{Again, the parameter
  blob $\pars$ should also be thought of as an immense, arbitrarily
  structured set or list of parameters, possibly even with no fixed
  dimension!} $\pars$, you can compute a pdf for data (or likelihood\note{We
  bash the terminology ``likelihood'' in [SOME OTHER NOTE] in this
  series.}) $p(\data\given\pars)$.
% ************* BJB
% <Wild Subjective Bayesian>
% The "generative model" is a probabilistic description of your *prior
% beliefs about what the data are going to be*. </Wild Subjective Bayesian>
% ************* Hogg
% That comment should go in the ``What is a Model?'' Chapter.
Furthermore, say you also can write down some kind of informative or
vague prior pdf $p(\pars)$ for the parameter blob.
If all these things are true, then---even if you can't compute
anything else---in principle a trivial-to-implement MCMC can give you
a fair sampling of the posterior pdf.
That is, you can run MCMC (for a very long time\note{Sometimes a very,
  very long time; we will return to that below.})  and you will be
left with a set of $K$ parameter-blob settings $\pars_k$ such that the
full set $\setof{\pars_k}_{k=1}^K$ constitutes a fair sampling from
the posterior pdf $p(\pars\given\data$).
% ************* BJB
% Notions of "sampling" and "Monte Carlo" are interesting. You might like to
% refer the interested reader to the paper "Monte Carlo is Fundamentally
% Unsound" by Tony O'Hagan.

All that said, and adhering to the traditions of the \project{Data
  Analysis Recipes} project\note{Every chapter in the \project{Data
    Analysis Recipes} series begins with a rant in which we argue that
  most uses of the methods in question are not appropriate!}, we are
compelled to note that MCMC is in fact \emph{over-used}.
Because MCMC provably (under assumptions\note{Hint at assumptions
  here.}, some of which will be discussed below) samples the full
posterior pdf in all of parameter space, many investigators use MCMC
because it will sample \emph{all} of the parameter space
($\pars$-space).
That is, they are using MCMC because they want to
\emph{search the parameter space for good models}.
Because MCMC samples the parameter representatively, it spends most of
its time near very good models; models that (within the confines of
the prior pdf) do a good job of explaining the data.
For this reason, many investigators are using MCMC because it
effectively \emph{optimizes the posterior pdf}, or, for certain
choices of prior pdf, \emph{optimizes the likelihood}.\note{Of course
  a committed Bayesian would argue that any time you are optimizing a
  posterior pdf or optimizing a likelihood, you \emph{should} be
  sampling a posterior pdf.  That is, for some, the fact that when
  someone ``wants'' to optimize, it is actually useful that they
  choose the ``wrong'' tool, because that ``wrong'' tool gives them
  back something far more useful than the output of any optimizer!}

Both of these reasons for using MCMC---that it is a parameter-space
search algorithm, and that it is a simple-to-code effective
optimizer---are \emph{not good reasons}.  MCMC is first and foremost a
\emph{sampler}.  If you are trying to find the optimum of the
likelihood or the posterior pdf, you should use an \emph{optimzer},
not a sampler.  If you want to make sure you search all of parameter
space, you should use a \emph{search algorithm}, not a sampler.  MCMC
is good at one thing, and one thing only: Sampling ill-normalized (or
otherwise hard to sample) pdfs.

\section{What is a sampling?}

Heuristically, a sampling $\setof{x_k}_{k=1}^K$ from some pdf $p(x)$
is a set of $K$ values $x_k$ that are independent draws from the pdf.
Heuristically, if an enormous (large $K$) sampling could be displayed
in a finely binned $x$-space histogram, the histogram would look---up
to a total normalization---just like the original function $p(x)$.

Not being mathematicians, we don't know the precise definition of a
pdf\note{Seriously.  We don't.}, but for our purposes here a pdf is
any single-valued (scalar) function that is non-negative everywhere
(the entire domain of $x$), and obeys a normalization condition
\begin{eqnarray}
0 &\leq& p(x) \quad \mbox{for all $x$}
\\
1 &=& \int p(x)\,\dd x
\quad ,
\end{eqnarray}
where, implicitly, the integral is over the full domain of $x$.
Importantly, although $p(x)$ is single-valued, $x$ can be a scalar or
a multi-element vector or list or blob; it can be arbitrarily large
and complicated.
In data-analysis contexts, $x$ will often be the full blob of free
parameters in the model.

Given this pdf $p(x)$, we can define \emph{expectation values} $E[x]$
for $x$ or for any quantity that can be expressed as a function $q(x)$
of $x$:
\begin{eqnarray}
E[x] &\equiv& \int x\,p(x)\,\dd x
\\
E[q] &\equiv& \int q(x)\,p(x)\,\dd x
\quad ,
\end{eqnarray}
where again the integrals are implicitly definite integrals over the
entire domain
of $x$ (all the parts of $x$ space in which $p(x)$ is finite) and the
integrals are multi-dimensional if $x$ is multi-dimensional.
These expectation values are the mean values of $x$ and $q(x)$ under
the pdf.  A good sampling---really the definition of a good
sampling---makes the sampling approximation to these integrals
accurate.
With a good sampling $\setof{x_k}_{k=1}^K$ the integrals get replaced
with sums over samples $x_k$:
\begin{eqnarray}
E[x] &\approx& \frac{1}{K}\,\sum_{k=1}^K x_k
\\
E[q] &\approx& \frac{1}{K}\,\sum_{k=1}^K q(x_k)
\quad .
\end{eqnarray}
That is, a sampling is good when any expectation value of interest is
accurately computed via the sampling approximation.
The word ``accurately'' here translates into some kinds of theorems
about limiting behavior; the general idea is that the sampling
approximation becomes exact as $K$ goes to infinity.
The size $K$ of the sampling you need \emph{in practice} will depend
on the expectations you want to compute, and the accuracies you need.

As we noted above, in the context of MCMC, we are often sampling or
using some \emph{badly normalized} function $f(x)$.
This function is just the pdf $p(x)$ multiplied by some unknown and
hard-to-compute scalar.
In this case, for our purposes, the conditions on $f(x)$ are that it
be non-negative everywhere and have \emph{finite} integral $Z$
\begin{eqnarray}
0 &\leq& f(x) \quad \mbox{for all $x$}
\\
Z &=& \int f(x)\,\dd x
\quad .
\end{eqnarray}

When the sampling $\setof{x_k}_{k=1}^K$ is of one of these badly
normalized functions $f(x)$---as it usually will be---the
sampling-approximation expectation values are the expectation values
under the properly-normalized corresponding pdf, even though you might
\emph{never learn that normalization}.
When we run MCMC sampling on $f(x)$, a function which differs from a
pdf $p(x)$ by some unknown normalization constant, then the sampling
permits computation of the following kinds of quantities:
\begin{eqnarray}
E[q] &\equiv& \frac{\int q(x)\,f(x)\,\dd x}{\int f(x)\,\dd x}
\\
E[q] &\approx& \frac{1}{K}\,\sum_{k=1}^K q(x_k)
\quad .
\end{eqnarray}
That is, the sampling can be constructed (as we will show below) from
$f(x)$ directly, and it permits you to compute expectation values
without ever requiring you to integrate either the numerator integral
or the denominator integral, both of which are generally
intractable.\note{As we will
  see, the intractability comes from various directions, but one is
  that the dimension of $x$ gets large in most realistic situations.
  Another is that the support of $p(x)$ tends to be, in normal
  inference situations, \emph{far smaller} than the full domain of
  $x$.
  That is, the pdf is at or very close to zero over all but some tiny
  and very hard-to-find part of the space.}

The ``correctness'' of a sampling is defined (above) in terms of its use in
  performing integrals or approximate computation of integrals.
In a deep sense, the \emph{only} thing a sampling is good for is
  computing integrals.
There are many uses of the MCMC sampling, some of them good and some of them bad.
Most of the good or sensible uses will somehow involve integration.

% in the next paragraph I adopt $d$ to be the dimensionality of the space

For example, one magical property of a sampling (in a $d$-dimensional space)
  is that a \emph{histogram} (a $d$-dimensional histogram) of the samples
  (divided, if you like, by the number of samples in each bin and the bin width\note{something about units and normalization})
  looks very much like the pdf from which the samples were drawn.
This is a way to ``reconstruct'' the pdf from the sampling:
Make a histogram from the samples.
Even in this case, the sampling is being used to do \emph{integrals};
  the (possibly odd) idea is that the approximate or effective value of the pdf in each bin
  of the histogram is an average over the bin.
That average is obtained by performing an integral.

Finally---and perhaps most importantly---another magical propert of a sampling
  (in a $d$-dimensional space)
  is that if some of your $d$ dimensions are totally uninteresting
  (nuisance parameters, if you will)
  and some of your $d$ dimensions are of great interest,
  the sampling in the full $d$-space is trivially converted into a sampling in the subspace of interest:
You just drop from each $x_k$ vector (or blob or list) the dimensions of no interest!
That is, the projection of the sampling to the subspace of interest produces
  a sampling of the marginalized pdf, marginalizing (or projecting) out the nuisance parameters.%
  \note{This point about marginalization is, once again, a use of the sampling to perform an \emph{integral};
        the marginalized pdf is obtained from the full pdf by an integration.
        The sampling performs this integration automatically.}
That is extremely important for inference,
  where there are always parameters with very different levels of importance to the scientific conclusions.
This point will return again below, either explicitly or implicitly. % does it?

\section{What is M--H MCMC?}

...The algorithm

...Why does it work (briefly)?

...What is a typical proposal distribution?

...Why everyone reading this document should stop here and JUST FREAKIN' CODE THIS UP.

\section{Ensembles, Hamiltonians, and other variants}

% *********** BJB talking:
% I suggest partitioning the discussion of different methods into the following
% two categories:
%
%  i) Methods for getting around the target distribution
% ii) Methods that change the target distribution to something
% other than the posterior pdf, to help the sampling somehow
%
% i) Would include things like M-H, the stretch move,
% Hamiltonian, slice sampling (maybe skip the latter)
% ii) would include things like annealing and Nested Sampling

%%%% Hogg talking:
% DFM: In this section, I think we should just summarize the other
% methods, not describe them in detail.  Then we can cite the relevant
% papers for the enthusiastic.

...What, briefly, is the stretch move, and why is it useful?

...What, briefly, is importance sampling?

...What, briefly, is nested sampling, and why is it good?

...What, briefly, is tempering?

...What, briefly, is hamiltonian, and why is it good?

...What, briefly, is reversible-jump, and why is it useful?

\section{Results, error bars, and figures}

For a committed probabilistic scientist---frequentist or
bayesian---there is no ``answer'', there are only probability density
functions (pdfs).  Any time in a paper or in an email or in a conversation
we say what ``the answer is''---even if we say it with an error
bar---we have departed the probabilistic program.  There are many
principled ways to depart the program; in another \documentname, we
discuss the economic explanation of how we can make hard decisions in
the context of probabilistic reasoning\note{CITE DECISION DOCUMENT}.
But without going into that ``economic'' model, it is fair to say that a
substantial problem with being a committed probabilist is that when we
\emph{publish}, we are not permitted (not now, at least) to publish a
\emph{probability distribution over pubications}; we have to publish
\emph{one single, deterministic text}; we have to make a decision about what
to write in the title, the abstract, the tables, figures, and results
section.  Given a MCMC sampling of the posterior pdf, what do we
report as our \emph{results}?

One thing we can keep in mind to guide us in this is what we said above,
  which is that samplings are good for doing \emph{integrals}.
We should endeavor to use as our ``results'' outputs from the MCMC
  that are based on integrals computed with the sampling.
This includes expectations, medians, quantiles, one-dimensional histograms,
  and multi-dimensional histograms.
This does \emph{not} include the ``best'' sample or a mode or optimum.
The latter things are not necessarily illegitimate outputs of the MCMC,
  but they do not make best use of the fact that the sampling is a tool for integration,
  and we do not recommend them.

Imagine that you are in the simplest case:
You have a model with a small number of parameters (say three-ish),
  and only one of them is of great interest.
What are your options for the ``measurement'' of this parameter?
The only simple integral-based options are the posterior mean
  or posterior median value for the parameter.
\\HOGG CHOOSE NOTATION AND GIVE FORMULAE HERE.

What are your best options for the ``uncertainty'' or ``limits'' on this parameter?
The only simple integral-based options are posterior quantiles.
That is, the ``one-sigma'' error bar can be the size of the central (or smallest%
  \note{HOGG shizzle about choosing quantiles}) interval of the parameter that contains 68 percent of the posterior samples.
\\HOGG GIVE FORMULA HERE.\\
The ``two-sigma'' error bar would be the same but for the 95-percent interval.

One amusing thing is that if you choose the 68-percent interval sensibly,
  but use as the ``measurement'' the posterior mean,
  you can get pathological situations in which the posterior mean measurement
  is actually \emph{outside} the one-sigma confidence interval.
For this reason (and others), we usually recommend as a default behavior%
  ---in the one-dimensional case---%
  to choose the \emph{median} of sampling as the measurement value,
  the 16-percent quantile as the lower error bar,
  and the 84-percent quantile as the upper error bar.
This has pathologies in higher dimensions (as we are about to see),
  but is pretty safe for one-dimensional answers.

The conservative scientist would show not just 68-percent error bars,
  but also 95-percent
  (to help readers visualize the skew of the posterior pdf).
The very conservative scientist would also show a histogram
  of the posterior samples,
  with the relvant quantiles (2.5, 16, 50, 84, and 97.5~percent) indicated.

As we mentioned above, scientists often like to report the ``best sample'';
  that is, the sample with the highest posterior pdf value.
This is not usually a good idea.
For one, MCMC is a sampling algorithm, not an optimization algorithm;
  there are no guarantees that it will find the optimum of the posterior pdf in reasonable time.%
  \note{The sampler samples the pdf fairly.
    The tallest peak in the posterior is not guaranteed to be---%
    and in general is not---%
    also the peak with the greatest posterior ``mass''.
    That is, the samples in a fair sampling will not necessarily come
    predominantly from the \emph{tallest} peak in the posterior pdf.
    They will come from the peak with the greatest total integral
    of the likelihood over the prior.
    These issues might sound ``academic'',
    but when the number of parameters gets large (greater than, say, 10),
    many normal intuitions one might have (if any) about what is reasonably likely in a pdf
    are regularly violated.}
For two, the closeness of the best sample to the posterior mode is a strong function of sample size,
  and with very bad scaling.%
  \note{If you have a $K$-point sampling in a $d$-dimensional parameter space,
    there is some best sample $\pars_k^{\best}$.  Now imagine that you want to make
    that best sample 10 times better---that is, you want to find a point $\pars^{\better}$
    that is ten times closer to the true optimum.  How many more samples do you need to take?
    In the limit that $K$ is large, on average, you have to take $10^d$ more samples!
    Most sensible optimizers are far, far better than this.}
For three, if you just want an optimum, use an optimizer!%
  \note{HOGG, something?}
That is, we \emph{strongly advise against} reporting,
  as ``the'' measurement or ``the result'',
  the parameter value or values for the best sample.
Giving some posterior samples is a good idea (see below).
If you feel drawn to give the best sample,
  instead give the parameter values found by optimizing the posterior PDF,
  starting at the best sample as an initialization
  (and sacrifice your probabilism\note{something}).
This optimal-posterior parameter value is called the
  ``maximum \foreign{a posteriori}'' or ``MAP'' value for the parameters;
  it is like a maximum-likelihood value but regularized by the prior.
It is an estimator with some good (and some bad) properties, but it is not the point of MCMC,
  and therefore outside the scope of this \documentname.

...Now for multiple dimensions...return to the marginalization point
from above...

...What do you do in high-dimensional spaces?  Note the issue
that---even in a unimodal posterior PDF---the most probable model
might not be at the set of parameter values that are reported if you
compute the parameter values to report in one-dimensional projections
of the posterior pdf.  This requires a figure to demonstrate.  What to
do here?

...What figures should you make to display the chain?

...If you are really good, you will pass forward a sampling.
...How big should it be? Refer to Mackay here.
...Usefulness of a big sampling for hierarchical inferences; concept of interim prior?

\section{Convergence}

A key question for an MCMC operator---\emph{the} key question in some
sense---is \emph{how long to run} to be sure of having reliable
results.
It is disappointing and annoying to many that there is no extremely
simple and reliable answer to this question.

The reason that there is no simple answer is that you can't really
ever know that you have sampled the full posterior pdf, and the reason
for \emph{that} is that if you \emph{could} know that, you would also
be able to solve the famously difficult discrete optimization
problem.\note{explain this remark and cite something here}
Another way to put it is that you might have two modes---two separated
regions of substantial total probability---in the posterior pdf that
are both important but separated by a large region of low probability.
If you are using a simple sampler, you could sample one of these modes
very well but the sampler will take effectively infinite time to find
the other mode.
Any test of convergence would be passed, but in fact the sampling
would be incomplete and not representative.
That is, knowing that you have a complete and representative sampling
is effectively impossible.

In this context,
  it is \emph{simply not fair} to require an MCMC run to have fully and completely
  sampled the posterior pdf,
  at least not in any provable sense.
The question of whether a sampling is converged to a representative sampling
  of the posterior pdf is actually outside the domain of science,
  because the domain of science is the domain of questions answerable by scientists.
That is, you can \emph{never know} that you have correctly sampled the posterior pdf,
  despite many statements in the literature to the contrary.%
  \note{Give example statements... Mention cases in which you \emph{might} be able to claim this...}
The upshot of this is that we don't and can't require any kind of absolute convergence;
  indeed, it would be impossible even to test for it.

In this pragmatic situation---the Real World, as it is called---we have to rely on heuristics.
Heuristically, you have sampled long enough when you can see
that the (or each) walker has traversed the high-probability parts of
the parameter space many times in the length of the chain.
Or, equivalently, you have sampled long enough when the first half of
the chain shows very much the same posterior pdf morphology as the
second half of the chain, or indeed as any substantial subset.

Making this heuristic condition more precise...
Autocorrelation time is the premier tool...
How is Autocorrelation time defined?..
How is Autocorrelation time measured?..
Why is it is not a panacea?..

...What plots should you make?
...Plots showing individual walkers going through the space.
...(now show plots that demonstrate a non-converged and a converged chain)...

\section{Tuning}

% BJB
% Please suggest mixtures of scales for M-H proposals.
% You can mimic the properties of slice sampling but with
% much easier code.

Most MCMC methods
  (though not all\note{We are promoters of an ensemble method that has a
  nearly-fixed proposal distribution and requires no tuning (CITE EMCEE).})
  make use of something like a ``proposal distribution''
  which determines what kinds of steps the walker can take
  as it random-walks through the parameter space.
The user generally has a lot of control over what this proposal distribution might be,
  and how to choose the parameters.
For example, in a $d$-dimensional parameter space, %do we consistently use d here?
  a zero-mean but anisotropic Gaussian (normal distribution)
  is often used to draw offsets for the walker to take from point to point.
Even within this choice (which is of one function among many possibilities),
  there are $d\,(d+1)/2$ parameters in the $d\times d$ symmetric, positive definite covariance matrix
  to set ``by hand''.
How to choose this function and set these parameters?

The key idea here is that if the proposal distribution is too narrow---%
  it proposes steps too small---%
  almost all steps will be accepted (REFERENCE ALGORITHM HERE),
  but it will take a long time to move anywhere because of timidity.
If the propsoal distribution is too wide---%
  it proposes steps too large---%
  the moves will cover parameter space easily,
  but almost no steps will be accepted;
  it will tend to jump to much lower probability regions.
There is a Goldilocks step size (proposal distribution root-variance) that is ``just right''.
In one dimension this might be easy to find,
  but, as we say, in large numbers of dimensions,
  there is a lot of freedom in choosing the parameters of the distribution.
  
In a deep sense, the only scalar that makes sense to optimize,
  when choosing proposal distribution (or, loosely speaking, step size),
  is the autocorrelation time, described above.
The optimal step size is the step size that makes for the shortest autocorrelation time.
This is easy to state, but hard to use in practice;
  being a second-order statistic of the MCMC chain,
  the autocorrelation time is a hard thing to measure without a lot of data,
  so it is hard to quickly estimate it and adjust,
  much less put it into some kind of optimization loop.
We usually use proxies for this of various kinds.

The simplest proxy for autocorrelation time optimization is the \emph{acceptance fraction}.
If you are accepting almost all proposed steps, your step sizes are too small on average.
If you are accepting almost none, your step sizes are too large.
The Goldilocks value is around a half,
  with a strong argument floating around that it should be 0.234 for best performance\note{CITE GELMAN}.
In the burn-in phase (discussed below) of an MCMC run,
  it makes sense to track the acceptance ratio,
  and adjust the proposal distribution variance as you get acceptance ratios
  that are far from the Goldilocks ratio.
This process can be automated easily;
  such automation is part of many projects that use MCMC.

A very common---%
  and very useful---%
  kind of proposal distribution is one that cycles through parameters,
  taking a random step in just one parameter at a time.
Aside from involving some persistence of state,
  this kind of proposal distribution can be valuable,
  in part because it reduces the (almost impossible) $d$-dimensional tuning problem
  to $d$ one-dimensional problems:
In this form of proposal distrubution,
  it is possible to track a separate acceptance fraction for every parameter.
Code can be built that uses the burn-in phase to tune all $d$ proposal variances
  such that each of them, individually,
  obtains acceptance at the same Goldilocks ratio.

One very important note to make here,
  because it is relevant to tuning,
  is that tuning can \emph{only} take place during the burn-in phase;
  you cannot tune \emph{while} you run your final MCMC run.
Why not?
Because tuning the proposal distribution based on the past history of the chain
  violates the ``Markov'' property
  that each step depends \emph{only} on the state at the previous step.
Violating the Markov property can be very bad;
  when you violate it,
  you lose all the provable properties of MCMC
  on which all our righteous power is based.

Another proxy for autocorrelation time is the
  Expected Squared Jump Distance.\note{(CITE GELMAN)}
This is the mean squared distance the walker moves, per step.
It is maximized when the acceptance ratio is reasonable
  and the step size is large;
  it is large when the exploration of the space is fast.
This proxy is easy to measure and use for tuning,
  and more directly related to autocorrelation time than the acceptance ratio.
We recommend using it for tuning, though we have never used it ourselves.
Like the acceptance fraction, it can also be used to tune
  in the case that the proposal distribution loops over parameters;
  once again, the user gets (in this case) $d$ one-dimensional tunings.

When sampling really gets very slow,
  there are various tricks and tips to work through.
One is to see if any of the parameters has a conditional posterior
  probability that is guaranteed to be Gaussian
  (this is true for some linear parameters in least-square problems
  when the priors are Gaussian, for example).
These dimensions can often be sampled \emph{exactly},
  outside of the MCCM framework;
  the algorithm looks like:
Take an MCMC step in the hard parameters,
  then take an exact step in the easy parameters,
  repeat.
Another related trick is to analytically marginalize,
  rather than sample,
  easy nuisance parameters.
The huge bag of possible tricks is so large,
  it goes way beyond the scope of this introductory \documentname.
In our experience, it is valuable to make friends with at least one statistician,
  one applied mathematician, and one computer scientist.
Between them, they ought to span the literature.

\section{Initialization and burn-in}

...Precede with optimization
% BJB
% If applicable. Bad idea in high dimension/underconstrained
% problem.

...Start from different locations
% BJB
% What different locations?
% Drawn from the prior can be useful. Then you can
% look at the chains and see which ones "made it" and
% which ones "got stuck" and delete the latter

...Dealing with multi-modal problems
% Starting from different locations is good
% for DIAGNOSING a problem as multimodal but it's
% terrible for quantifying the modes. Explain why,
% it's a good teaching point.

\section{Likelihoods and priors}

MCMC is used to obtain samples $x_k$ from pdf $p(x)$,
  or a badly normalized all-positive function $f(x)$
  that is different from the pdf by an unknown factor $Z$.
In the context of data analysis,
  MCMC is usually being used to obtain samples
  that are \emph{parameter values} for a probabilistic model of some data.
That is, MCMC is sampling a pdf \emph{for the parameters}.
If it is returning parameter values, then it is not sampling a pdf for the data.
That is, MCMC can only sample a posterior (or prior) pdf for the parameters.
\emph{It cannot sample a likelihood.}%
\note{Well, technically, MCMC \emph{can} be used to sample a likelihood function,
  but the samples would be samples of possible \emph{data} not possible \emph{parameters}.
  The purist might say that even this is not sampling a likelihood function,
  because you should only call it a ``likelihood function'' in contexts in which you are
  treating the data as fixed (at the true data values) and the parameters as variable.}

Despite all this, in many cases,
  data analysts believe they are sampling the likelihood.
This is because (we presume) they have put a likelihood function
  (or log-likelihood function)
  in as the input to the MCMC code, where the probability function
  (or log-probability function) should go.
Then is it the likelihood that is being sampled?
No, not really;
it is a posterior probability that is directly proportional to the likelihood function.
That is, it is a posterior probability for some implicit (and improper) ``flat'' priors.

...Give a sense of the difficulties you get into if you fail here; like
discussion with Sven Kreiss on 2013-02-14...

...Explain how your code \emph{should look} with a prior function and a likelihood function...

...It is a good idea to have proper priors.
It is a good test of your priors that you can sample with a ``null'' likelihood function.
You can often have your priors wrong despite having beautiful code.

...You must describe your priors in the same parameterization that you ``step'' in.
That is, if you change your parameters, you \emph{must} change your priors correspondingly.
Give the example of going from $A$, $\phi$ to $A\,\cos\phi$, $A\,\sin\phi$.

\section{Troubleshooting}

\paragraph{functional testing:}
...Run on standard distributons.

...Set likelihood to 1 and check that you sample your prior pdf.

\paragraph{low acceptance fraction:}
If you find that you are getting low acceptance fraction
  (in standard M-H MCMC or other varieties for which there is such a concept)
  you can simply reduce the sizes of the steps you consider to increase the fraction that are accepted.
This means decreasing the variance of the proposal distribution,
  or whatever parameter or parameters control the width of that distribution.
If reducing the variance of the proposal distribution does \emph{not} increase the acceptance fraction,
  then \emph{you have a bug}.
You must find that bug by auditing your code or else returning to the functional tests listed above.

Similarly for high acceptance fraction:
If you are finding a high acceptance fraction,
  then increasing the variance of the proposal distributon \emph{must} decrease the acceptance fraction.
If it does \emph{not}, then \emph{you have a bug}.

\paragraph{sticky chains:}
...Appearance of good and bad chains (show examples).

\paragraph{initialization-dependence:}
...Different initializations leading to different posterior samplings?

...Jammed ensemble sampler?

\paragraph{parameterization problems:}
...Degeneracies and duplications in parameter space; reparameterizations.

\paragraph{model checking:}
...Beyond the scope of this document...
Something about cross-validation and posterior predictive checks...
Something about plotting models (drawn from posterior) on data, where appropriate...
How could you possibly know that your sampling really represents your \emph{real} posterior PDF?..

\section{Advice and discussion}

...Write and debug your own M--H sampler and then download a real
package \emph{afterwards}.
% ***** BJB
% I AGREE WHOLEHEARTEDLY. That's how to learn.

...Simple problems where the outputs are analytic.  Functional testing.

...breaking Markov-ness.
% ***** BJB
% "Diminishing adaptations" is an idea that is quite easy
% to explain. There are other ways to break Markov-ness but
% this probably covers enough of them for an intro document
% ***** Hogg
% I was going to say you should NEVER break Markov-ness
% unless you REALLY understand what you are doing.

...Priors: Once again:
If you describe your priors in one parameterization but
then perform sampling in a different space, it is easy to make
mistakes.  For example: Flat prior in amplitude and phase, sample in
$A\,\cos\phi$ and $A\,\sin\phi$.  There are many ways to do this
wrong.

...Sample the prior (likelihood set to unity) to check priors and parameterization (another functional test).

...Posteriors: On the other hand, you can do anything you like (within
reason) to the output samples and be sampling the posterior in that
transformed quantity.

...How to deal with forbidden regions in prior, or sharp prior boundaries?

...How to deal with constraints that pin you to a subspace of the parameter space?  Method-dependent.

...Initialization and burn-in advice we haven't addressed earlier?

...The Bayes integral / evidence integral?

...Posterior predictive checks?

...Finally, to repeat: You can't sample the likelihood!  You can only
sample a posterior pdf!

% \begin{figure}[htbp]
% \exampleplot{ex17}
% \caption{Partial solution to \problemname~\ref{prob:bayesintrinsic}:
% The marginalized posterior probability distribution for the intrinsic
% variance.}\label{fig:bayesintrinsic}
% \end{figure}

\clearpage
\markright{Notes}\theendnotes

\clearpage
\begin{thebibliography}{}\markright{References}
\bibitem[Foreman-Mackey et al.(2012)]{emcee}
  Foreman-Mackey,~D., Hogg,~D.~W., Lang,~D., \& Goodman,~J.\ 2012,
  \project{emcee}: The MCMC Hammer,
  \arxiv{1202.3665}
\end{thebibliography}

\end{document}
