% to-do
% -----
% - write zeroth draft
% - write first draft
% - send to friendlies for comments
%   - Lang
%   - Rix
%   - Bovy
% - post on arXiv

% style notes
% -----------
% - [LaTeX] newline after every full stop for proper git diffing
% - [LaTeX] eqnarray not equation
% - pdf not PDF

\documentclass[12pt,twoside,pdftex]{article}
\usepackage{styles/dar_endnotes}

\newcommand{\this}{Using Markov Chain Monte Carlo}
\include{styles/dar}

\newcommand{\data}{D}
\newcommand{\pars}{\theta}

\begin{document}

\thispagestyle{plain}\raggedbottom
\section*{Data analysis recipes:\\ \this\footnotemark}

\footnotetext{%
    The notes begin on page~\pageref{note:first}, including the
    license\note{\label{note:first}
        Copyright 2013 by the authors. This work is licensed under a
        \href{http://creativecommons.org/licenses/by-nc-nd/3.0/deed.en\_US}{%
            Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported
            License}.}
    and the acknowledgements\note{%
        We would like to thank
          Jo Bovy (IAS),
          Brendon Brewer (Auckland),
          Jonathan Goodman (NYU),
          Fengji Hou (NYU), and
          Dustin Lang (CMU)
        for valuable advice and comments.}.
}

\noindent
Dan~Foreman-Mackey\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics,
       New York University}\\[1ex]
David~W.~Hogg\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics,
       New York University}\\
\affil{Max-Planck-Institut f\"ur Astronomie, Heidelberg}

\begin{abstract}
Markov Chain Monte Carlo (MCMC) methods for sampling probability
distribution functions, plus abundant computational resources,
have transformed the sciences.
Here we give a fast overview of basic MCMC methods and then turn to
practical advice for their use in real inference problems.
We give advice on method choice, tuning for performance,
initialization and burn-in, judging convergence, and use of the chain
output to produce parameter estimates with associated uncertainties.
[Insert some generally useful point here!]
\end{abstract}

\section{When do you need MCMC?}

Markov Chain Monte Carlo (MCMC) methods are methods for sampling
probability distribution functions or probability density functions (pdfs).
These pdfs may be either probability mass functions on a discrete
space, or probability densities on a continuous space, though we will
concentrate on the latter in this \documentname.
MCMC methods don't require that you have a full analytic description of the
properly normalized pdf for sampling to proceed; they only require
that you be able to compute ratios of the pdf at pairs of locations.
This makes MCMC methods ideal for sampling \emph{posterior
  pdfs} in probabilistic inferences:

In a probabilistic inference, the posterior pdf $p(\pars\given\data)$,
or pdf for the parameters $\pars$ given the data $\data$, is
constructed from the likelihood $p(\data\given\pars)$, or pdf for the
data given the parameters, and the prior pdf $p(\pars)$ for the
parameters by what's often known as ``Bayes rule'',
\begin{eqnarray}
p(\pars\given\data) &=& \frac{1}{Z}\,p(\data\given\pars)\,p(\pars)
\quad .
\end{eqnarray}
In these contexts, the constant $Z$, sometimes written as
$p(\data)$, is known by the names ``evidence'', ``marginal likelihood''
the ``Bayes integral'' and ``prior predictive probability'', and is usually
\emph{extremely hard to
  calculate}.\note{Say words about why we call it $Z$ not $p(\data)$
  and why it is extremely hard to calculate.}
That is, you often know the function $p(\pars\given\data)$ up to a
constant factor; you can compute ratios of the pdf at pairs of points,
but not the precise value at any individual point.

In addition to this normalization-insensitive property of MCMC, in its
simplest forms it can be run without computing any derivatives or
integrals of the function, and (as we will show below) in its simplest
forms it is \emph{extremely easy to implement}.
For all these reasons, MCMC is ideal for sampling posterior pdfs in
the real situations in which scientists find themselves.

Say you are in this situation.
You have a huge blob of data $\data$ (think of this as a vector or
list or heterogeneous collection of observations\note{In general in
  this \documentname\ we will treat any variable as being capable of
  containing not just a value but some huge arbitrarily structured set
  of values in matrix, vector, or more complex form.}).
You also have a model sophisticated enough---a probabilistic,
generative model, if you will\note{For our definition of a
  \emph{model}, see [SOME OTHER NOTE] in this series.}---that, given a
setting of a huge blob of parameters\note{Again, the parameter
  blob $\pars$ should also be thought of as an immense, arbitrarily
  structured set or list of parameters, possibly even with no fixed
  dimension!} $\pars$, you can compute a pdf for data (or likelihood\note{We
  bash the terminology ``likelihood'' in [SOME OTHER NOTE] in this
  series.}) $p(\data\given\pars)$.
% ************* BJB
% <Wild Subjective Bayesian>
% The "generative model" is a probabilistic description of your *prior
% beliefs about what the data are going to be*. </Wild Subjective Bayesian>
% ************* Hogg
% That comment should go in the ``What is a Model?'' Chapter.
And say you also can write down some kind of informative or
vague prior pdf $p(\pars)$ for the parameter blob.
If all these things are true, then---even if you can't compute
anything else---in principle a trivial-to-implement MCMC can give you
a fair sampling of the posterior pdf.
That is, you can run MCMC (for a very long time\note{Sometimes a very,
  very long time; we will return to that below.})  and you will be
left with a set of $K$ parameter-blob settings $\pars_k$ such that the
full set $\setofall{\pars_k}_{k=1}^K$ constitutes a fair sampling from
the posterior pdf $p(\pars\given\data$).
% ************* BJB
% Notions of "sampling" and "Monte Carlo" are interesting. You might like to
% refer the interested reader to the paper "Monte Carlo is Fundamentally
% Unsound" by Tony O'Hagan.

All that said, and adhering to the traditions of the \project{Data
  Analysis Recipes} project\note{Every chapter in the \project{Data
    Analysis Recipes} series begins with a rant in which we argue that
  most uses of the methods in question are not appropriate!}, we are
compelled to note that MCMC is in fact \emph{over-used}.
Because MCMC provably (under assumptions\note{Hint at assumptions
  here.}, some of which will be discussed below) samples the full
posterior pdf in all of parameter space, many investigators use MCMC
because it will sample \emph{all} of the parameter space
($\pars$-space).
That is, they are using MCMC because they want to
\emph{search the parameter space for good models}.
Because MCMC samples the parameter representatively, it spends most of
its time near very good models; models that (within the confines of
the prior pdf) do a good job of explaining the data.
For this reason, many investigators are using MCMC because it
effectively \emph{optimizes the posterior pdf}, or, for certain
choices of prior pdf, \emph{optimizes the likelihood}.\note{Of course
  a committed Bayesian would argue that any time you are optimizing a
  posterior pdf or optimizing a likelihood, you \emph{should} be
  sampling a posterior pdf.  That is, for some, the fact that when
  someone ``wants'' to optimize, it is actually useful that they
  choose the ``wrong'' tool, because that ``wrong'' tool gives them
  back something far more useful than the output of any optimizer!}

Both of these reasons for using MCMC---that it is a parameter-space
search algorithm, and that it is a simple-to-code effective
optimizer---are \emph{not good reasons}.  MCMC is first and foremost a
\emph{sampler}.  If you are trying to find the optimum of the
likelihood or the posterior pdf, you should use an \emph{optimzer},
not a sampler.  If you want to make sure you search all of parameter
space, you should use a \emph{search algorithm}, not a sampler.  MCMC
is good at one thing, and one thing only: Sampling ill-normalized (or
otherwise hard to sample) pdfs.

\section{What is M--H MCMC?}

...The algorithm

...Why does it work (briefly)?

...What is a typical proposal distribution?

...You can't sample the likelihood!  You can only sample a posterior
pdf!
% *********** BJB
% YES!!!! Although you can sample the likelihood if your samples are in
% data space. :)

% *********** BJB
% I suggest partitioning the discussion of different methods into the following
% two categories:
%
%  i) Methods for getting around the target distribution
% ii) Methods that change the target distribution to something
% other than the posterior pdf, to help the sampling somehow
%
% i) Would include things like M-H, the stretch move,
% Hamiltonian, slice sampling (maybe skip the latter)
% ii) would include things like annealing and Nested Sampling

\section{stretch move and other variants}

...How does the stretch move work?

...Why is the stretch move useful?

...What, briefly, is nested sampling, and why is it good?

...What, briefly, is hamiltonian, and why is it good?

\section{Results, error bars, and figures}

...So you have a chain output, what do you do with it?

...What should you report as the inferred parameter value and
associated uncertainty (error bar)?  What is an upper limit?

...What figures should you make to check the chain?

\section{Convergence}

...How do you know you have run long enough?  Heuristically...

...Autocorrelation time

\section{Tuning}

...How do you make it run fast?

...Acceptance ratio and ESJD and etc.

% BJB
% Please suggest mixtures of scales for M-H proposals.
% You can mimic the properties of slice sampling but with
% much easier code.

...Gibbs sampling and interior marginalizations

\section{Initialization and burn-in}

...Precede with optimization
% BJB
% If applicable. Bad idea in high dimension/underconstrained
% problem.

...Start from different locations
% BJB
% What different locations?
% Drawn from the prior can be useful. Then you can
% look at the chains and see which ones "made it" and
% which ones "got stuck" and delete the latter

...Dealing with multi-modal problems
% Starting from different locations is good
% for DIAGNOSING a problem as multimodal but it's
% terrible for quantifying the modes. Explain why,
% it's a good teaching point.

\section{Advice and discussion}

...Write and debug your own M--H sampler and then download a real
package \emph{afterwards}.
% ***** BJB
% I AGREE WHOLEHEARTEDLY. That's how to learn.

...breaking Markov-ness.
% ***** BJB
% "Diminishing adaptations" is an idea that is quite easy
% to explain. There are other ways to break Markov-ness but
% this probably covers enough of them for an intro document
% ***** Hogg
% I was going to say you should NEVER break Markov-ness
% unless you REALLY understand what you are doing.

...Initialization and burn-in advice we haven't addressed earlier?

...The Bayes integral / evidence integral?

...Posterior predictive checks?

...Finally, to repeat: You can't sample the likelihood!  You can only
sample a posterior pdf!

% \begin{figure}[htbp]
% \exampleplot{ex17}
% \caption{Partial solution to \problemname~\ref{prob:bayesintrinsic}:
% The marginalized posterior probability distribution for the intrinsic
% variance.}\label{fig:bayesintrinsic}
% \end{figure}

\clearpage
\markright{Notes}\theendnotes

\clearpage
\begin{thebibliography}{}\markright{References}
\bibitem[Foreman-Mackey et al.(2012)]{emcee}
  Foreman-Mackey,~D., Hogg,~D.~W., Lang,~D., \& Goodman,~J.\ 2012,
  \project{emcee}: The MCMC Hammer,
  \arxiv{1202.3665}
\end{thebibliography}

\end{document}
