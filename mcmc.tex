% This file is part of the MCMC project,
%   which is part of the Data Analysis Recipes project
% Copyright 2013, 2014, 2015 David W. Hogg (NYU) and Dan Foreman-Mackey (NYU).

% to-do
% -----
% - mention JAGS and STAN in methods section
% - perhaps re-order to m-h, convergence, reporting results, other methods ??
% - where to put comments about how to structure code ln_likelihood(pars, data, I) and ln_prior(pars, I)
% - need to mention case where there is an MC integral INSIDE the likelihood function
% - need to mention testing the likelihood function
% - send to friendlies for comments
%   - Lang
%   - Rix
%   - Bovy
%   - Brewer
%   - Hill
% - post on arXiv

% style notes
% -----------
% - These documents are ``Chapters'', no?
% - [LaTeX] newline after every full stop for proper git diffing
% - [LaTeX] eqnarray not equation
% - pdf not PDF
% - is it ``a MCMC sampling'' or ``an MCMC sampling''?
% - \note{} right after word if you are endnoting a phrase,
%   but after the full stop if endnoting the sentence or paragraph.
% - do we work in $x$ or $\pars$?  I am confused...
% - should we use the word ``walker'' anywhere, everywhere, or never?

\documentclass[12pt,twoside,pdftex]{article}
\usepackage{styles/dar_endnotes}

\newcommand{\this}{Using Markov Chain Monte Carlo}
\include{styles/dar}

\newcommand{\data}{D}
\newcommand{\pars}{\theta}
\newcommand{\best}{{\mathrm{(best)}}}
\newcommand{\better}{{\mathrm{(better)}}}

\begin{document}

\thispagestyle{plain}\sloppy\sloppypar\raggedbottom
\section*{Data analysis recipes:\\ \this\footnotemark}

\footnotetext{%
    The notes begin on page~\pageref{note:first}, including the
    license\note{\label{note:first}
        Copyright 2013, 2014 by the authors. This work is licensed under a
        \href{http://creativecommons.org/licenses/by-nc-nd/3.0/deed.en\_US}{%
            Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported
            License}.}
    and the acknowledgements\note{%
        We would like to thank
          Jo Bovy (IAS),
          Brendon Brewer (Auckland),
          Jonathan Goodman (NYU),
          Fengji Hou (NYU), and
          Dustin Lang (CMU)
        for valuable advice and comments.}.
}

\noindent
Dan~Foreman-Mackey\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics,
       New~York~University}\\[1ex]
David~W.~Hogg\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics,
       New~York~University}\\
\affil{Center~for~Data~Science,
       New~York~University}\\
\affil{Max-Planck-Institut~f\"ur~Astronomie, Heidelberg}

\begin{abstract}
Markov Chain Monte Carlo (MCMC) methods for sampling probability
distribution functions, plus abundant computational resources,
have transformed the sciences.
Here we give a fast overview of basic MCMC methods and then turn to
practical advice for their use in real inference problems.
We give advice on method choice, tuning for performance,
initialization and burn-in, judging convergence, and use of the chain
output to produce parameter estimates with associated uncertainties.
[Insert some generally useful point here!]
\end{abstract}

\section{When do you need MCMC?}

Markov Chain Monte Carlo (MCMC) methods are methods for sampling
probability distribution functions or probability density functions (pdfs).
These pdfs may be either probability mass functions on a discrete
space, or probability densities on a continuous space, though we will
concentrate on the latter in this \documentname.
MCMC methods don't require that you have a full analytic description of the
properly normalized pdf for sampling to proceed; they only require
that you be able to compute ratios of the pdf at pairs of locations.
This makes MCMC methods ideal for sampling \emph{posterior
  pdfs} in probabilistic inferences:

In a probabilistic inference, the posterior pdf $p(\pars\given\data)$,
or pdf for the parameters $\pars$ given the data $\data$, is
constructed from the likelihood $p(\data\given\pars)$, or pdf for the
data given the parameters, and the prior pdf $p(\pars)$ for the
parameters by what's often known as ``Bayes rule'',
\begin{eqnarray}
p(\pars\given\data) &=& \frac{1}{Z}\,p(\data\given\pars)\,p(\pars)
\label{eq:bayes}\quad .
\end{eqnarray}
In these contexts, the constant $Z$, sometimes written as
$p(\data)$, is known by the names ``evidence'', ``marginal likelihood''
the ``Bayes integral'' and ``prior predictive probability'', and is usually
\emph{extremely hard to
  calculate}.\note{Some would call
  the factor $Z$ in \equationname~(\ref{eq:bayes}) something like
  ``$p(\data)$'' or ``the marginal likelihood'' or the 
  probability of the data.
  Of course it is only the probability of the data within the model
  space under consideration!
  We discuss this object and these issues elsewhere (HOGG CITE).
  The factor $Z$ is often hugely difficult to compute, because the
  likelihood (or the prior) can have extremely complex structure,
  with multiple arbitrarily compact modes, arbitrarily positioned
  in the (presumably high dimensional) parameter space $\pars$.
  Elsewhere, we discuss the computation of this object (HOGG cite Fengji)
  and---perhaps more importantly---why you probably never really
  \emph{want} to compute it (HOGG CITE).
  The computation of $Z$ is outside the scope of this \documentname,
  but suffice it to say:
  If the computation of $Z$ is in your travel plans, you will need to
  think about implementing some non-trivial integrating or sampling
  technology; we will give some brief description of options in
  \sectionname~\ref{sec:methods}.}
That is, you often know the function $p(\pars\given\data)$ up to a
constant factor; you can compute ratios of the pdf at pairs of points,
but not the precise value at any individual point.

In addition to this normalization-insensitive property of MCMC, in its
simplest forms it can be run without computing any derivatives or
integrals of the function, and (as we will show below) in its simplest
forms it is \emph{extremely easy to implement}.
For all these reasons, MCMC is ideal for sampling posterior pdfs in
the real situations in which scientists find themselves.

Say you are in this situation.
You have a huge blob of data $\data$ (think of this as a vector or
list or heterogeneous collection of observations\note{In general in
  this \documentname\ we will treat any variable as being capable of
  containing not just a value but some huge arbitrarily structured set
  of values in matrix, vector, or more complex form.}).
You also have a model sophisticated enough---a probabilistic,
generative model, if you will\note{For more discussion
  of our definition of a \emph{model}, see [HOGG SOME OTHER NOTE] in
  this series.
  Briefly, a ``model'' for us is a likelihood function (a pdf for the data given
  model parameters), and a prior pdf over the parameters.
  Because, under this definition, the model can always generate (by
  sampling, say) parameters and parameters can generate (again by
  sampling, say) data, the model is effectively (or actually, if you
  are a true subjective Bayesian) a probability distribution (pdf)
  over all possible data.
}---that, given a
setting of a huge blob of parameters\note{Again, the parameter
  blob $\pars$ should also be thought of as an immense, arbitrarily
  structured set or list of parameters, possibly even with no fixed
  dimension!} $\pars$, you can compute a pdf for data (or likelihood\note{We
  bash the terminology ``likelihood'' in [SOME OTHER NOTE] in this
  series.}) $p(\data\given\pars)$.
Furthermore, say you also can write down some kind of informative or
vague prior pdf $p(\pars)$ for the parameter blob.
If all these things are true, then---even if you can't compute
anything else---in principle a trivial-to-implement MCMC can give you
a fair sampling of the posterior pdf.
That is, you can run MCMC
(for a very long time---see \sectionname~\ref{sec:convergence} for how long)
and you will be left with a set of $K$ parameter-blob settings $\pars_k$ such that the
full set $\setof{\pars_k}_{k=1}^K$ constitutes a fair sampling from
the posterior pdf $p(\pars\given\data$).
% ************* BJB
% Notions of "sampling" and "Monte Carlo" are interesting. You might like to
% refer the interested reader to the paper "Monte Carlo is Fundamentally
% Unsound" by Tony O'Hagan.

All that said, and adhering to the traditions of the \project{Data
  Analysis Recipes} project\note{Every \documentname\ in the \project{Data
    Analysis Recipes} series begins with a rant in which we argue that
  most uses of the methods in question are not appropriate!}, we are
compelled to note that MCMC is in fact \emph{over-used}.
Because MCMC provably (under assumptions\note{Hint at assumptions
  here.}, some of which will be discussed below) samples the full
posterior pdf in all of parameter space, many investigators use MCMC
because it will sample \emph{all} of the parameter space
($\pars$-space).
That is, they are using MCMC because they want to
\emph{search the parameter space for good models}.
Because MCMC samples the parameter representatively, it spends most of
its time near very good models; models that (within the confines of
the prior pdf) do a good job of explaining the data.
For this reason, many investigators are using MCMC because it
effectively \emph{optimizes the posterior pdf}, or, for certain
choices of prior pdf, \emph{optimizes the likelihood}.\note{Of course
  a committed Bayesian would argue that any time you are optimizing a
  posterior pdf or optimizing a likelihood, you \emph{should} be
  sampling a posterior pdf.  That is, for some, the fact that when
  someone ``wants'' to optimize, it is actually useful that they
  choose the ``wrong'' tool, because that ``wrong'' tool gives them
  back something far more useful than the output of any optimizer!}

Both of these reasons for using MCMC---that it is a parameter-space
search algorithm, and that it is a simple-to-code effective
optimizer---are \emph{not good reasons}.  MCMC is first and foremost a
\emph{sampler}.  If you are trying to find the optimum of the
likelihood or the posterior pdf, you should use an \emph{optimzer},
not a sampler.  If you want to make sure you search all of parameter
space, you should use a \emph{search algorithm}, not a sampler.  MCMC
is good at one thing, and one thing only: Sampling ill-normalized (or
otherwise hard to sample) pdfs.

In what follows, we are going to provide a kind of ``user manual'' or
advice document or folklore capture regarding the use of MCMC for
data analysis.
This will not be a detailed description of multiple MCMC methods
(indeed, we will only explain one method in detail), and it will not
be about the mathematical properties or structure of the method or
methods.
It will be about how to use MCMC, including diagnosis and
trouble-shooting.

The first couple of \sectionname s will describe what a sampling is, and how
the simplest MCMC method, the Metropolis-Hastings algorithm, can
provide one.
The next few \sectionname s will provide ideas about how to
initialize, tune and operate MCMC methods for good performance.
The last few \sectionname s will provide advice for making decisions
among the myriad MCMC methods implementations, how to implement a good
likelihood function and prior pdf function for inference, and how to
trouble-shoot standard kinds of problems that arise in operating MCMC
methods on real problems.
We will leave parenthetical and philosophical matters to the end-notes,
which appear after the main text.

\section{What is a sampling?}

Heuristically, a sampling $\setof{x_k}_{k=1}^K$ from some pdf $p(x)$
is a set of $K$ values $x_k$ that are independent draws from the pdf.
Heuristically, if an enormous (large $K$) sampling could be displayed
in a finely binned $x$-space histogram, the histogram would look---up
to a total normalization---just like the original function $p(x)$.

Not being mathematicians, we don't know the precise definition of a
pdf\note{Seriously.  We don't.  It has something to do with measure theory.}, but for our purposes here a pdf is
any single-valued (scalar) function that is non-negative everywhere
(the entire domain of $x$), and obeys a normalization condition
\begin{eqnarray}
0 &\leq& p(x) \quad \mbox{for all $x$}
\\ \label{eq:normp}
1 &=& \int p(x)\,\dd x
\quad ,
\end{eqnarray}
where, implicitly, the integral is over the full domain of $x$.
Importantly, although $p(x)$ is single-valued, $x$ can be a scalar or
a multi-element vector or list or blob; it can be arbitrarily large
and complicated.
In data-analysis contexts, $x$ will often be the full blob of free
parameters in the model.
Implicitly, the integral in \equationname~(\ref{eq:normp}) is high-dimensional; it
has as many dimensions as there are elements or entries or components of $x$.
Also, if there are elements or entries or components of $x$ that are discrete
(that is, take on only integer values or equivalent),
then along those dimensions the integral becomes a discrete sum.
This latter is a detail to which we return below.

Given this pdf $p(x)$, we can define \emph{expectation values} $E[x]$
for $x$ or for any quantity that can be expressed as a function $q(x)$
of $x$:
\begin{eqnarray}
E[x] &\equiv& \int x\,p(x)\,\dd x
\\
E[q] &\equiv& \int q(x)\,p(x)\,\dd x
\quad ,
\end{eqnarray}
where again the integrals are implicitly definite integrals over the
entire domain
of $x$ (all the parts of $x$ space in which $p(x)$ is finite) and the
integrals are multi-dimensional if $x$ is multi-dimensional.
These expectation values are the mean values of $x$ and $q(x)$ under
the pdf.  A good sampling---really the definition of a good
sampling---makes the sampling approximation to these integrals
accurate.
With a good sampling $\setof{x_k}_{k=1}^K$ the integrals get replaced
with sums over samples $x_k$:
\begin{eqnarray}
E[x] &\approx& \frac{1}{K}\,\sum_{k=1}^K x_k
\\
E[q] &\approx& \frac{1}{K}\,\sum_{k=1}^K q(x_k)
\quad .
\end{eqnarray}
That is, a sampling is good when any expectation value of interest is
accurately computed via the sampling approximation.
The word ``accurately'' here translates into some kinds of theorems
about limiting behavior; the general idea is that the sampling
approximation becomes exact as $K$ goes to infinity.
The size $K$ of the sampling you need \emph{in practice} will depend
on the expectations you want to compute, and the accuracies you need.

As we noted above, in the context of MCMC, we are often sampling or
using some \emph{badly normalized} function $f(x)$.
This function is just the pdf $p(x)$ multiplied by some unknown and
hard-to-compute scalar.
In this case, for our purposes, the conditions on $f(x)$ are that it
be non-negative everywhere and have \emph{finite} integral $Z$
\begin{eqnarray}
0 &\leq& f(x) \quad \mbox{for all $x$}
\\
Z &=& \int f(x)\,\dd x
\quad .
\end{eqnarray}

When the sampling $\setof{x_k}_{k=1}^K$ is of one of these badly
normalized functions $f(x)$---as it usually will be---the
sampling-approximation expectation values are the expectation values
under the properly-normalized corresponding pdf, even though you might
\emph{never learn that normalization}.
When we run MCMC sampling on $f(x)$, a function which differs from a
pdf $p(x)$ by some unknown normalization constant, then the sampling
permits computation of the following kinds of quantities:
\begin{eqnarray}
E[q] &\equiv& \frac{\int q(x)\,f(x)\,\dd x}{\int f(x)\,\dd x}
\\
E[q] &\approx& \frac{1}{K}\,\sum_{k=1}^K q(x_k)
\quad .
\end{eqnarray}
That is, the sampling can be constructed (as we will show below) from
$f(x)$ directly, and it permits you to compute expectation values
without ever requiring you to integrate either the numerator integral
or the denominator integral, both of which are generally
intractable.\note{As we will
  see, the intractability comes from various directions, but one is
  that the dimension of $x$ gets large in most realistic situations.
  Another is that the support of $p(x)$ tends to be, in normal
  inference situations, \emph{far smaller} than the full domain of
  $x$.
  That is, the pdf is at or very close to zero over all but some tiny
  and very hard-to-find part of the space.}

The ``correctness'' of a sampling is defined (above) in terms of its use in
  performing integrals or approximate computation of integrals.
In a deep sense, the \emph{only} thing a sampling is good for is
  computing integrals.
There are many uses of the MCMC sampling, some of them good and some of them bad.
Most of the good or sensible uses will somehow involve integration.

% in the next paragraph I adopt $d$ to be the dimensionality of the space

For example, one magical property of a sampling (in a $d$-dimensional space)
  is that a \emph{histogram} (a $d$-dimensional histogram) of the samples
  (divided, if you like, by the number of samples in each bin and the bin width\note{something about units and normalization})
  looks very much like the pdf from which the samples were drawn.
This is a way to ``reconstruct'' the pdf from the sampling:
Make a histogram from the samples.
Even in this case, the sampling is being used to do \emph{integrals};
  the (possibly odd) idea is that the approximate or effective value of the pdf in each bin
  of the histogram is an average over the bin.
That average is obtained by performing an integral.

Integrals are also involved in finding the mean, median, and any quantiles
  of a pdf.
They are not involved in finding the \emph{mode} of a pdf.
For this reason (and others), in what follows, when we talk about what to report
  about the outcome of your MCMC sampling,
  we will advise in favor of mean, median, and quantiles,
  and we will advise against mode.

Finally---and perhaps most importantly---a magical property of a sampling
  (in a $d$-dimensional space)
  is that if some of your $d$ dimensions are totally uninteresting
  (nuisance parameters, if you will)
  and some of your $d$ dimensions are of great interest,
  the sampling in the full $d$-space is trivially converted into a sampling in the subspace of interest:
You just drop from each $x_k$ vector (or blob or list) the dimensions of no interest!
That is, the projection of the sampling to the subspace of interest produces
  a sampling of the marginalized pdf, marginalizing (or projecting) out the nuisance parameters.%
  \note{This point about marginalization is, once again, a use of the sampling to perform an \emph{integral};
        the marginalized pdf is obtained from the full pdf by an integration.
        The sampling performs this integration automatically.}
That is extremely important for inference,
  where there are always parameters with very different levels of importance to the scientific conclusions.
This point will return again below, either explicitly or implicitly. % HOGG: does it?

Although the discussion in this \documentname\ is general, the most
common use of MCMC sampling (for us, anyway) is in probabilistic
inference.
For this reason, we will often refer to the function $f(\pars)$
colloquially as ``the posterior pdf''\note{We will also sometimes
  refer to expectations under $f(\pars)$ as posterior means, and
  medians as median-of-posterior values, and so on.} even though it is
implicitly ill-normalized and might not be a posterior pdf in any
sense.
We will also occasionally assume---just because it is true in
inference---that the function $f(\pars)$ is the product of two
functions, one called ``the prior pdf'' and one called ``the
likelihood''.
Again, this usage is colloquial and is only strictly correct in
inference contexts with proper inputs.
That the function $f(\pars)$ can be thought of as a product of a
prior pdf and a likelihood is only necessary for what follows in
the context of advanced sampling techniques like tempering or
nested sampling, both mentioned briefly below.

\section{What is Metropolis--Hastings MCMC?}

The simplest algorithm for MCMC is the Metropolis--Hastings algorithm (M--H MCMC).\note{%
  HOGG put here some short discussion of the name of the algorithm and the original reference.}
It is so simple that we recommend that any reader of this document
  who has not previously implemented the algorithm take a break at the end
  of this \sectionname\ and implement it forthwith, in a short piece of computer code,
  in the context of some simple problem.\note{%
    HOGG put reference to fitting a line here and point inside to the simple problem suggestion.}

The M--H MCMC algorithm requires two inputs.
The first is a handle to the function $f(\pars)$ that is the function to be sampled,
  such that the algorithm can evaluate $f(\pars)$ for any value of the parameters $\pars$.
In data-analysis contexts,
  this function would be the prior $p(\pars)$ times the likelihood $p(\data\given\pars)$
  evaluated at the observed data $\data$.
The second input is a proposal pdf $q(\pars'\given\pars)$ that can be sampled,
  such that the algorithm can draw a new position $\pars'$ in the parameter space
  given an ``old'' position $\pars$.
This second function must meet a symmetry requirement (detailed
  balance) we discuss further below.
It permits us to random-walk around the parameter space in a fair way.

The algorithm is the following:
We have generated some set of samples, the most recent of which is $\pars_k$
To generate the next sample $\pars_{k+1}$ do the following:
\begin{itemize}
\item Draw a proposal $\pars'$ from the proposal pdf $q(\pars'\given\pars_k)$.
\item Draw a random number $0<r<1$ from the uniform distribution.
\item If $f(\pars') / f(\pars_k) > r$ then $\pars_{k+1} \leftarrow \pars'$;
      otherwise $\pars_{k+1} \leftarrow \pars_k$.
\end{itemize}
That is, at each step, either a new proposed position in the parameter
  space gets accepted into the list of samples or else the previous sample
  in the parameter space gets repeated.
The algorithm can be iterated a large number $K$ of times to produce $K$ samples.

This algorithm---and indeed any MCMC algorithm---produces a
  \emph{biased random walk} through parameter space.
It is a random walk for the same reason that it is ``Markov'':
The step it makes to position $\pars_{k+1}$ depends only on the state
  of the sampler at position $\pars_k$ (and no previous state).
It is a biased random walk, biased by the acceptance algorithm
  involving the ratios of function values; this acceptance rule biases
  the random walk such that the amount of time spent in the neighborhood
  of location $\pars$ is proportional to $f(\pars)$.
Because of this local (Markov) property, the nearby samples are not
  independent; the algorithm only produces fair samples in the limit of
  arbitrary run time, and two samples are only independent when they are
  sufficiently separated in the chain (more on this below).

The principal user-settable knob in M--H MCMC is the proposal
  pdf $q(\pars'\given\pars)$.
A typical choice is a multi-variate Gaussian distribution for $\pars'$
  centered on $\pars$ with some simple (diagonal, perhaps) variance
  tensor.
We will discuss the choice and tuning of this proposal distribution
  below.

Importantly---for the algorithm given above to work correctly---the
  proposal pdf must satisfy a ``detailed balance'' condition; it
  must have the property that
\begin{eqnarray}
q(\pars'\given\pars) &=& q(\pars\given\pars')
\quad ;
\end{eqnarray}
that is, it must be just as easy to go one way in the parameter space
  as the other.
You can break this property, if you like, and then adjust the
  acceptance condition accordingly, but we \emph{do not recommend this}
  except under the supervision of a trained professional.\note{Actually,
  our own favorite method, \project{emcee}, has a proposal pdf that does
  violate detailed balance in this way and has a compensated
  acceptance ratio.  Read the paper [HOGG CITE] for more details.}
If the detailed balance condition frightens you (as it frightens us),
  then just stick with pdfs that are symmetric in $\pars'$ and $\pars$,
  like the Gaussian, or a centered uniform distribution and forget about
  it.

In what follows, when we discuss tuning of the proposal pdf, we will
  often cycle through the parameter blob components and propose changes
  in only one dimension or element or component at a time.
That is, at step $k+1$ you might only propose a one-dimensional move
  in the $i$th dimension, not in all $d$ dimensions of the parameter
  space.
That won't change anything significant; but if you are implementing
  this right now you might want to do that.
It will help us tune the proposal pdf (\sectionname~\ref{sec:tuning})
and diagnose problems (\sectionname~\ref{sec:troubleshooting}).

...Why does M--H MCMC work (briefly)? ...Or put this in a note on an earlier paragraph?

One note to make here
  is that you must \emph{propose}
  in the same coordinates (parameterization or transformation of parameters)
  as that in which your priors are \emph{specified},
  or else multiply in a (possibly nasty) Jacobian.
That is, if your prior is ``flat in $x$''
  but you find it easier to run your sampler by proposing steps in $\ln x$,
  if you don't modify your acceptance fraction by the Jacobian,
  your \emph{real} prior won't be flat in $x$.
These think-os can get subtle;
  the best way to avoid them
  is to write your code and your likelihood function
  and your prior pdf function and your proposal distribution
  all in precisely the same parameterization.
We will return to this point below in our comments on functional testing.

As we discuss briefly below in \sectionname~\ref{sec:methods},
  there are many MCMC methods more advanced than M--H.
However, they all share characteristics with M--H
  (initialization, tuning, judging convergence),
  such that it is very valuable to understand M--H well before using anything more advanced.
Furthermore, a scientist new to MCMC benefits enormously from
  building, tuning, and using her or his own MCMC software.
One piece of advice we give then,
  to the new user of MCMC,
  is to code up, tune, and use a M--H MCMC sampler for a scientific project.
A huge amount is learned in doing this,
  and it is very often the case that the home-built M--H MCMC does everything needed;
  you often don't need any more advanced tool.
Only after you have concluded that your home-built M--H MCMC sampler
  is not suited to your project
  (or not developed properly into a properly versatile software package)
  should you download and start to use any professionally developed alternative.
That is, even if---in the end---%
  you want to leave the sampling code to the experts and run an industrial-strength code,
  it is still valuable to build your own,
  given the simplicity of the algorithm,
  and given the intuition you gain by doing it (at least once) yourself.

\section{Convergence}\label{sec:convergence}

A key question for an MCMC operator---\emph{the} key question in some
sense---is \emph{how long to run} to be sure of having reliable
results.
It is disappointing and annoying to many that there is no extremely
simple and reliable answer to this question.

The reason that there is no simple answer is that you can't really
ever know that you have sampled the full posterior pdf, and the reason
for \emph{that} is that if you \emph{could} know that, you would also
be able to solve the famously difficult discrete optimization
problem.\note{explain this remark and cite something here}
Another way to put it is that you might have two modes---two separated
regions of substantial total probability---in the posterior pdf that
are both important but separated by a large region of low probability.
If you are using a simple sampler, you could sample one of these modes
very well but the sampler will take effectively infinite time to find
the other mode.
Any test of convergence would be passed, but in fact the sampling
would be incomplete and not representative.
That is, knowing that you have a complete and representative sampling
is effectively impossible.

In this context,
  it is \emph{simply not fair} to require an MCMC run to have fully and completely
  sampled the posterior pdf,
  at least not in any provable sense.
The question of whether a sampling is converged to a representative sampling
  of the posterior pdf is actually outside the domain of science,
  because the domain of science is the domain of questions answerable by scientists.
That is, you can \emph{never know} that you have correctly sampled the posterior pdf,
  despite many statements in the literature to the contrary.%
  \note{Give example statements... Mention cases in which you \emph{might} be able to claim this...}
The upshot of this is that we don't and can't require any kind of absolute convergence;
  indeed, it would be impossible even to test for it.

In this pragmatic situation---the Real World, as it is called---we have to rely on heuristics.
Heuristically, you have sampled long enough when you can see
that the (or each) walker has traversed the high-probability parts of
the parameter space many times in the length of the chain.
Or, equivalently, you have sampled long enough when the first half of
the chain shows very much the same posterior pdf morphology as the
second half of the chain, or indeed as any substantial subset.

The above heuristics can be made more precise (in terms of the amount
of deviation one expects between the means and variances, say, of two
disjoint subsets of the chain.
The premier tool for making this heuristic precise, however, is to
look at the \emph{autocorrelation time} of the chain.
In general, when one has a sequence $(\pars_1, \pars_2, \theta_3, \cdots)$
generated by a Markov Process in the $\pars$ space, nearby points in
the sequence will be similar, but sufficiently distant points will not
``know about'' one another;
the autocorrelation function measures this.

The autocorrelation function is defined by
\begin{eqnarray}
C(\Delta) &\equiv& \frac{E[(\pars_k - \mu)\,(\pars_{k+\Delta} - \mu)]}{E[(\pars - \mu)^2]}
\label{eq:autocorrelationfn}\\
\mu &=& E[\pars]
\quad ;
\end{eqnarray}
that is, it is the covariance of points separated by $\Delta$, taken
relative to the variance.
In the above equation, the assumption is that the samples $\pars_k$ are
given in the order that they were generated by the MCMC algorithm.
By construction
\begin{eqnarray}
C(0) &=& 1
\\
\lim_{\Delta\rightarrow\infty} C(\Delta) &=& 0
\quad ,
\end{eqnarray}
and by definition, the autocorrelation ``time'' $\tau$ is
\begin{eqnarray}
\tau = \sum_{\Delta = 0}^{\infty} C(\Delta)
\label{eq:autocorrelationtime}\quad .
\end{eqnarray}
Heuristically, the autocorrelation time is the number of samples you
need to take before you get an independent sample.
More precisely, the autocorrelation time is the factor by which you
have to reduce the number of samples when you compute the variance on
a sampling-based expectation value estimate.\note{HOGG Explain more fully.  Maybe also cite Fengji.}
It is slightly off-topic here, but related to this, if you have a long
chain of length $K$ and with autocorrelation time $\tau$ and you thin
the chain down to every $\tau$th sample (instead of using all $K$
samples), you will get just as good sampling-based estimates as if you
had used all $K$ samples.

A sampler with a better autocorrelation time is just better; you have
to do fewer $f(\pars)$ calls per independent sample, and you have to
run less time to get accurate sampling-based integral estimates.
A sampler that takes an independent sample every time would have an
autocorrelation time of unity, which is the best possible value; this
optimal sampling is only possible for problems where the sampling is
analytic (for example if the posterior is perfectly Gaussian with
known mean and variance).
In general, the best sampler will be different for different problems,
and we will (below) tune the samplers we have to do the best on the
problems we have; this tuning will also be problem-specific.
There are many heuristic bases on which different samplers might be
compared or tuned, but fundamentally it is lower autocorrelation time
that separates good samplers from bad ones and is the ultimate basis
on which we compare performance.\note{HOGG: Bash Dunkley and anyone
  else who has used something else to compare.}

Of course, the autocorrelation-time definition
(\ref{eq:autocorrelationtime}) does not by itself provide an
\emph{estimator} for the time.
Usually, we just take the naive estimator---the ratio of empirical
expectations implied by \equationname~(\ref{eq:autocorrelationfn}),
and then perform a naive sum to get the autocorrelation time.
However, almost all empirical estimators of the autocorrelation time
will end up being under-estimates (biased low), because the finite
length of the chain makes any subset seem more representative than it
truly is; indeed this is a sub-problem of the problem that convergence
can never be known empirically with good confidence.

Finally one last point about convergence:
Since all convergence tests are fundamentally heuristic, it is useful
to just make the heuristic visualization of the samples $\pars_k$ as a
function of $k$, in the order they were generated.
The chain is likely to be converged only if the random-walk process
crossed the domain of $\pars$ fully many times during the MCMC run.
Often some parameter directions are much worse than others; it is
worth looking at the chain in all parameter directions.

\section{Tuning}\label{sec:tuning}

Most MCMC methods
  (though not all\note{We are promoters of an ensemble method that has a
  nearly-fixed proposal distribution and requires no tuning (CITE EMCEE).})
  make use of something like a ``proposal distribution''
  which determines what kinds of steps the walker can take
  as it random-walks through the parameter space.
The user generally has a lot of control over what this proposal distribution might be,
  and how to choose the parameters.
For example, in a $d$-dimensional parameter space, %do we consistently use d here?
  a zero-mean but anisotropic Gaussian (normal distribution)
  is often used to draw offsets for the walker to take from point to point.
Even within this choice (which is of one function among many possibilities),
  there are $d\,(d+1)/2$ parameters in the $d\times d$ symmetric, positive definite covariance matrix
  to set ``by hand''.
How to choose this function and set these parameters?

The key idea here is that if the proposal distribution is too narrow---%
  it proposes steps too small---%
  almost all steps will be accepted (REFERENCE ALGORITHM HERE),
  but it will take a long time to move anywhere because of timidity.
If the propsoal distribution is too wide---%
  it proposes steps too large---%
  the moves will cover parameter space easily,
  but almost no steps will be accepted;
  it will tend to jump to much lower probability regions.
There is a Goldilocks step size (proposal distribution root-variance) that is ``just right''.
In one dimension this might be easy to find,
  but, as we say, in large numbers of dimensions,
  there is a lot of freedom in choosing the parameters of the distribution.
  
% BJB
% Please suggest mixtures of scales for M-H proposals.
% You can mimic the properties of slice sampling but with
% much easier code.

In a deep sense, the only scalar that makes sense to optimize,
  when choosing proposal distribution (or, loosely speaking, step size),
  is the autocorrelation time, described above.
The optimal step size is the step size that makes for the shortest autocorrelation time.
This is easy to state, but hard to use in practice;
  being a second-order statistic of the MCMC chain,
  the autocorrelation time is a hard thing to measure without a lot of data,
  so it is hard to quickly estimate it and adjust,
  much less put it into some kind of optimization loop.
We usually use proxies for this of various kinds.

The simplest heuristic proxy statistic for tuning is the \emph{acceptance fraction}.
If you are accepting almost all proposed steps, your step sizes are too small on average.
If you are accepting almost none, your step sizes are too large.
The Goldilocks value is around a quarter,
  with an argument floating around that it should be 0.234 for best performance\note{CITE GELMAN}
  (though you could never tune it precisely enough to warrant that third digit of accuracy).
In the burn-in phase (discussed below) of an MCMC run,
  it makes sense to track the acceptance ratio,
  and adjust the proposal distribution variance as you get acceptance ratios
  that are far from the Goldilocks ratio.
This process can be automated easily;
  such automation is part of many projects that use MCMC.

% HOGG:  Is the following a repeat from previously and/or does it mesh with what's earlier?

A very common---%
  and very useful---%
  kind of proposal distribution is one that cycles through parameters,
  taking a random step in just one parameter at a time.
Aside from involving some persistence of state,
  this kind of proposal distribution can be valuable,
  in part because it reduces the (almost impossible) $d$-dimensional tuning problem
  to $d$ one-dimensional problems:
In this form of proposal distrubution,
  it is possible to track a separate acceptance fraction for every parameter.
Code can be built that uses the burn-in phase to tune all $d$ proposal variances
  such that each of them, individually,
  obtains acceptance at the same Goldilocks ratio.

One note to make here---%
  because it is relevant to tuning---%
  is that tuning can \emph{only} take place during the burn-in phase;
  you cannot tune \emph{while} you run your final MCMC run.
Why not?
Because tuning the proposal distribution based on the past history of the chain
  violates the ``Markov'' property
  that each step depends \emph{only} on the state at the previous step.
Violating the Markov property can be very bad;
  when you violate it,
  you lose all the provable properties of MCMC
  on which all our righteous power is based.

Another proxy for autocorrelation time useful for tuning is the
  Expected Squared Jump Distance.\note{(CITE GELMAN)}
This is the mean squared distance the walker moves, per step.
It is maximized when the acceptance ratio is reasonable
  and the step size is large;
  it is large when the exploration of the space is fast.
This proxy is easy to measure and use for tuning,
  and more directly related to autocorrelation time than the acceptance ratio.
We recommend using it for tuning, though we have never used it ourselves.
Like the acceptance fraction, it can also be used to tune
  in the case that the proposal distribution loops over parameters;
  once again, the user gets (in this case) $d$ one-dimensional tunings.

When sampling really gets very slow,
  there are various tricks and tips to work through.
One is to see if any of the parameters has a conditional posterior
  probability that is guaranteed to be Gaussian
  (this is true for some linear parameters in least-square problems
  when the priors are Gaussian, for example).
These dimensions can often be sampled \emph{exactly},
  outside of the MCCM framework;
  the algorithm looks like:
Take an MCMC step in the hard parameters,
  then take an exact step in the easy parameters,
  repeat.
Another related trick is to analytically marginalize,
  rather than sample,
  easy nuisance parameters.
The huge bag of possible tricks is so large,
  it goes way beyond the scope of this introductory \documentname.
In our experience, it is valuable to make friends with at least one statistician,
  one applied mathematician, and one computer scientist.
Between them, they ought to span the literature.

\section{Initialization and burn-in}

Just as most (though not all) MCMC methods require a choice of proposal distribution,
  most (though not all) require a choice about initialization:
The walker needs (or walkers need) to be started somewhere in the parameter space.
In many use cases for MCMC,
  the investigator wants to use it to obtain uncertainty information about
  or propagate uncertainty into parameter estimates.
In these cases, it makes sense to initialize the walker or walkers at sensible parameter estimates,
  found by optimizing a likelihood or a posterior PDF in advance of sampling.
In extremely high dimensions (large numbers of parameters), this can be a bad idea,
  since the optimal parameters are not necessarily near typical posterior samples\note{Make the counterintuitive point.},
  but in low dimensions (few to tens) this is often sensible.

In other use cases,
  the investigator wants to use MCMC to search all of parameter space
  for good models, and also sample.
In this case, it makes sense to be more catholic with initialization...

...but note that MCMC is not the ideal search algorithm for any parameter space.

...Start from different locations
% BJB
% What different locations?
% Drawn from the prior can be useful. Then you can
% look at the chains and see which ones "made it" and
% which ones "got stuck" and delete the latter

...Dealing with multi-modal problems
% Starting from different locations is good
% for DIAGNOSING a problem as multimodal but it's
% terrible for quantifying the modes. Explain why,
% it's a good teaching point.

\section{Results, error bars, and figures}

For a committed probabilistic scientist---frequentist or
Bayesian---there is no ``answer'', there are only probability density
functions (pdfs).  Any time in a paper or in an email or in a conversation
we say what ``the answer is''---even if we say it with an error
bar---we have departed the probabilistic program.  There are many
principled ways to depart the program; in another \documentname, we
discuss the economic explanation of how we can make hard decisions in
the context of probabilistic reasoning\note{CITE DECISION DOCUMENT}.
But without going into that ``economic'' model, it is fair to say that a
substantial problem with being a committed probabilist is that when we
\emph{publish}, we are not permitted (not now, at least) to publish a
\emph{probability distribution over pubications}; we have to publish
\emph{one single, deterministic text}; we have to make a decision about what
to write in the title, the abstract, the tables, figures, and results
section.  Given a MCMC sampling of the posterior pdf, what do we
report as our \emph{results}?

One thing we can keep in mind to guide us in this is what we said above,
  which is that samplings are good for doing \emph{integrals}.
We should endeavor to use as our ``results'' outputs from the MCMC
  that are based on integrals computed with the sampling.
This includes expectations, medians, quantiles, one-dimensional histograms,
  and multi-dimensional histograms.
This does \emph{not} include the ``best'' sample or a mode or optimum.
The latter things are not necessarily illegitimate outputs of the MCMC,
  but they do not make best use of the fact that the sampling is a tool for integration,
  and we do not recommend them.

Imagine that you are in the simplest case:
You have a model with a small number of parameters (say three-ish),
  and only one of them is of great interest.
What are your options for the ``measurement'' of this parameter?
The only simple integral-based options are the posterior mean
  or posterior median value for the parameter.

For example,
  imagine that by MCMC you have generated
  $K$ samples $\pars_k$ of a parameter vector $\pars$.
Now you have a scalar function $q(\pars)$ which takes the parameter vector
  and returns a scalar value%
  \note{For the median-of-sampling value, $q(\pars)$ needs to return a scalar,
  but technically, for the mean-of-sampling, $q(\pars)$ can be a vector or something high-dimensional.}
  which could be as simple as a single component of the parameter vector
  (one parameter from the list),
  or something more complex.
The mean-of-sampling value $\mean{q}$ is just
\begin{eqnarray}
\mean{q} &\leftarrow& \frac{1}{K}\,\sum_{k=1}^K q(\pars_k)
\quad ,
\end{eqnarray}
  and the median-of-sampling value is just the $[K/2]$th value of $q(\pars_k)$
  when the $q(\pars_k)$ have been ordered (sorted).
These are both produced by integrals;
  the first is the value returned by the expectation integral estimate,
  the second is the value past which half of the integrated pdf lies.

What are your best options for the ``uncertainty'' or ``limits'' on this parameter?
The only simple integral-based options are either variances or else posterior quantiles.
We usually use quantiles.
That is, the ``one-sigma'' error bar can be taken to be the half-size of the central (or smallest%
  \note{HOGG shizzle about choosing quantiles}) interval of the parameter
  that contains 68 percent of the posterior samples.
The ``two-sigma'' error bar would be the same but for the 95-percent interval.
Again, these limits can be estimated%
  \note{One amusing thing about samplings,
  which are so beloved of us probabilistic (Bayesian) reasoners,
  is that anything we \emph{do} with a sampling,
  like estimate an integral or a quantile,
  is just an old-school frequentist estimator.
  A frequentist estimator of a Bayesian quantity, to be sure, but a frequentist estimator nonetheless.}
  by ordering the $q(\pars_k)$ values,
  and finding the $\pars$ values such that some fraction (0.68 or 0.95)
  of the samples lie between them.
Again, to form these limits, $q(\pars)$ must be a scalar function (so sorting is well defined).

One amusing thing is that if you choose the 68-percent interval sensibly,
  but use as the ``measurement'' the posterior mean,
  you can get pathological situations in which the posterior mean measurement
  is actually \emph{outside} the one-sigma confidence interval.
For this reason (and others), we usually recommend as a default behavior%
  ---in the one-dimensional case---%
  to choose the \emph{median} of sampling as the measurement value,
  the 16-percent quantile as the lower one-sigma error bar,
  and the 84-percent quantile as the upper one-sigma error bar.
This has pathologies in higher dimensions (as we are about to see),
  but is pretty safe for one-dimensional answers.

The conservative scientist would show not just 68-percent error bars,
  but also 95-percent
  (to help readers visualize the skew of the posterior pdf).
The very conservative scientist would also show a histogram
  of the posterior samples,
  with the relvant quantiles (2.5, 16, 50, 84, and 97.5~percent) indicated.

As we mentioned above, scientists often like to report the ``best sample'';
  that is, the sample with the highest posterior pdf value.
This is not usually a good idea.
For one, MCMC is a sampling algorithm, not an optimization algorithm;
  there are no guarantees that it will find the optimum of the posterior pdf in reasonable time.%
  \note{The sampler samples the pdf fairly.
    The tallest peak in the posterior is not guaranteed to be---%
    and in general is not---%
    also the peak with the greatest posterior ``mass''.
    That is, the samples in a fair sampling will not necessarily come
    predominantly from the \emph{tallest} peak in the posterior pdf.
    They will come from the peak with the greatest total integral
    of the likelihood over the prior.
    These issues might sound ``academic'',
    but when the number of parameters gets large (greater than, say, 10),
    many normal intuitions one might have (if any) about what is reasonably likely in a pdf
    are regularly violated.}
For two, the closeness of the best sample to the posterior mode is a strong function of sample size,
  and with very bad scaling.%
  \note{If you have a $K$-point sampling in a $d$-dimensional parameter space,
    there is some best sample $\pars_k^{\best}$.  Now imagine that you want to make
    that best sample 10 times better---that is, you want to find a point $\pars^{\better}$
    that is ten times closer to the true optimum.  How many more samples do you need to take?
    In the limit that $K$ is large, on average, you have to take $10^d$ more samples!
    Most sensible optimizers are far, far better than this.}
For three, if you just want an optimum, use an optimizer!%
  \note{HOGG, something?}
That is, we \emph{strongly advise against} reporting,
  as ``the'' measurement or ``the result'',
  the parameter value or values for the best sample.
Giving some posterior samples is a good idea (see below).
If you feel drawn to give the best sample,
  instead give the parameter values found by optimizing the posterior PDF,
  starting at the best sample as an initialization
  (and sacrifice your probabilism\note{something}).
This optimal-posterior parameter value is called the
  ``maximum \foreign{a posteriori}'' or ``MAP'' value for the parameters;
  it is like a maximum-likelihood value but regularized by the prior.
It is an estimator with some good (and some bad) properties, but it is not the point of MCMC,
  and therefore outside the scope of this \documentname.

The parameter space $\pars$ is usually multi-dimensional, and more
than one of those parameters is of interest.
In this case, the mean or median of sampling can be produced for each
dimension.
Because the sampling projected onto any dimension is a marginalization
of the posterior pdf, any such one-dimensional mean or median is the
mean or median of the marginalized posterior pdf.

There is one oddity to note here, as it catches many investigators by
surprise:
Even if the model is good and the posterior pdf is unimodal and well-behaved,
the median (or even mean) of the posterior pdf for each of parameters,
when taken together as a ``best-fit'' parameter vector $\pars^\ast$,
will not itself necessarily be a good fit to the data!
If there is substantial ``curvature'' to the pdf in the parameter
space, the mean or median of sampling does not necessarily lie at a
high-probability location of parameter space.
This may seem counterintuitive, but it is easy to see in the case of a
``banana-shaped'' posterior pdf.
This all relates to the fact that MCMC is not an optimizer, it is a
sampler.
It is important, when reporting the output of an MCMC run by giving
means or medians of the posterior PDF to remind the reder or user of
that output that it does not necessarily represent (collectively) a
good fit to the data; these outputs only give information that is
useful one parameter at a time.

In principle the only output from the sampling that safely gives both
probabilistic information about the result of the inference and also
good-fitting models is a few---randomly chosen---example samples.
We strongly recommend this\note{HOGG:  And we do this WHERE? Citation!};
in particular this is a better thing to do
than to give the ``best sample'', which isn't even guaranteed to be
near the bulk of the posterior pdf or the samples therefrom.

If you \emph{do} return posterior samples to your audience, how many
should you return?
There are only heuristic answers to this, but a guideline is that, if
you only care about a single (one-dimensional) parameter, you don't
need many samples; a dozen suffices to give you a reasonable posterior
pdf mean and variance.\note{HOGG cite Mackay.}
If you care about $K$ parameters or dimensions of the parameter space,
in general the need for samples grows exponentially (or perhaps even
factorially!) with the number of dimensions.\note{HOGG cite something.}
So if you have a ten-dimensional space, expect to be publishing your
samples in an electronic table on the web!
If you \emph{do} publish sufficient numbers of posterior samples, you
might find users who want to make serious use of them.\note{HOGG cite
  our importance-sampling papers with some explanation here.}

Of course figures are always better than tables, at least in the
printed form of scientific publications.
In a $D$-dimensional parameter space, we recommend plotting all $D$
one-dimensional sample histograms, on a pretty fine binning, and all
$D$-choose-2 two-dimensional histograms (scatter plots) to show the
low-level covariances and non-linearities.
If you are clever, all these plots can be arranged into a lovely and
informative triangle.\note{HOGG cite triangle.py and put an example in
  the note!}
These two-dimensional plots do not in any sense contain all the
information in the sampling!  However, they are remarkable for
locating expected and unexpected parameter relationships, and often
invaluable for suggesting re-parameterizations and transformations
that simplify your problem.

\section{Gibbs, ensemble, Hamiltonian, and so on}\label{sec:methods}

As we have discussed above, MCMC methods are proven to be correct ``in the limit''; that is, when
an infinite number of samples have been taken.
When the function $f(\pars)$ has challenging properties, the results
can approach this limit very slowly (or not at all\note{HOGG: discuss
  situations in which MCMC will not work---related to support.}).
For this reason, in the finite duration of a scientific project, most
users of MCMC have one of two (related) problems:
Either \textsl{(1)}~it is taking too long---it is taking too many steps or
executing too many calls of the function $f(\pars)$ to get independent samples---or \textsl{(2)}~it is not
exploring the full parameter space---there are local optima (or bad
local geometry around those optima) ``trapping'' the algorithm.

There is no general, problem-independent solution for either of these
problems.
Indeed, there are many different strategies that work well or badly
for different functions $f(\pars)$.
In this \sectionname, we will try to guide the reader towards methods
that might help in different circumstances.
We will not give a full description of all the MCMC algorithms out
there, but just provide some field notes and pointers to references.
New MCMC methods are being developed all the time, so a reader with a
serious problem would do well to consult with the applied-mathematics
or statistics literature.\note{HOGG: Your best interface to these
  literatures is a colleague in another department.  Break down those walls!}
The different methods to which we refer here are (in general) very
different in their difficulty of implementation.
We won't help with that either, except to say that there are more and
more open-source or distributed packages every year.
We will try to name some current implementations in the notes, but any
list compiled today will be incomplete tomorrow.
The spaces of MCMC methods and MCMC method implementations are both
growing rapidly, in part because MCMC has become such a core
technology in the empirical sciences.

...Note a good general reference here?  MacKay?

% *********** BJB talking:
% I suggest partitioning the discussion of different methods into the following
% two categories:
%
%  i) Methods for getting around the target distribution
% ii) Methods that change the target distribution to something
% other than the posterior pdf, to help the sampling somehow
%
% i) Would include things like M-H, the stretch move,
% Hamiltonian, slice sampling (maybe skip the latter)
% ii) would include things like annealing and Nested Sampling

%%%% Hogg talking:
% DFM: In this section, I think we should just summarize the other
% methods, not describe them in detail.  Then we can cite the relevant
% papers for the enthusiastic.

\paragraph{Ensembles:}
As we discuss more below, one of the critical challenges in the use of
MCMC is tuning the proposal pdf $q(\pars'\given\pars)$.
In problems in which all of the parameters (all of the components or entries of $\pars$) are somehow ``equivalent'',
or the parameters can be seen as components of a vector in a vector
space, there are \emph{ensemble methods} that make use of not just one
random-walker but instead many walkers to automatically generate a properly tuned
proposal distribution.
These methods make use of the distribution of a set of independent
walkers in the parameter space ($\pars$-space) to gauge the typical step-sizes and
directions at which new proposals should be made, obviating or greatly
reducing the tuning requirements.
We are co-authors on one of these methods, \project{emcee}\note{HOGG:
  ref emcee}, which is popular in the astrophysical sciences.
Because ensemble methods are updating independent MCMC chains, they
are often easy to parallelize to make use of multiple cores.

\paragraph{Gibbs:}
In many problems, the parameters---the components or entries of the
blob $\pars$---are not equivalent at all.
Some are global parameters, which touch or have an effect on every
sub-part of the data blob $\data$, while others are local parameteres,
which only touch a small subset of the data.
Or, in another use case, some parameters are linear parameters
that---at fixed values of the other parameters---will have a Gaussian
likelihood or posterior pdf.
In these problems, there are some directions in parameter space that
are hard to move in (they require complete re-calculation of the full
non-linear function $f(\pars)$, and other directions that are very
easy to move in (they require only partial re-calculations or the
sampling could even be analytic and exact).
For problems with these structures, Gibbs samplers are ideal, and
there are good implementations.\note{HOGG cite JAGS}

A trivial kind of Gibbs sampling was mentioned above in the
discussion of M--H MCMC:
Even when running vanilla M--H MCMC, it is often useful to update only
one parameter at a time; that is, do only axis-aligned moves in the
$\pars$-space, cycling through axes as you go from step to step.
This helps with tuning and diagnosis, but it also permits a good
implementation to capitalize on simpler calculations for the simpler
parameter directions.

Gibbs sampling can be very good for exploiting multiprocessing:
Global parameter updates might require a serial re-calculation of the
full likelihood, but local parameter updates can be done in parallel,
with each local parameter working on its local bit of data all at the
same time.
This kind of structure is common in hierarchical inference, where
local parameters touch only individual objects in some population,
say, and global parameters are parameters \emph{of} that population as
a whole.
Parallelization is built in to some Gibbs implementations.\note{HOGG}

\paragraph{Hamiltonian:}
[HOGG, DFM: THIS HAS SOME OTHER NAME; WHAT?]
In many cases of interest, the function $f(\pars)$ can be analytically
differentiated, such that it is possible to quickly execute a gradient
$\dd f/\dd\pars$ evaluation.
Indeed, in the modern world of computing, there are even
auto-differentiation systems that deliver automatically code that
produces the gradient of any function you can write.\note{HOGG cite
  autodiff systems}
There is a class of MCMC methods that make use of gradient information
to effectively inform the proposal distribution and thus speed sampling.

... HOGG more here!

\paragraph{Importance:}
Sometimes it is possible to sample efficiently from a distribution
that is close to the target function $f(\pars)$ you care about but
hard to sample efficiently with MCMC.
An example is a problem that has a simple (Gaussian, say) likelihood
function times a non-trivial prior:
It is easy to sample from a Gaussian likelihood times a Gaussian
prior, even without MCMC, but it might be hard to do the same sampling
under a more complex prior.
Importance sampling is a direct sampling method (not really an MCMC
method at all):
You take a sampling from the approximate function (which by assumption
was easy) and then you re-weight the sampling (or do a subsequent
rejection of samples) using the ratio of the true function to the
approximate function as a weight or probability.
We use this quite a bit in our hierarchical inferences.\note{HOGG:
  Citations}

Two of the significant technical details for importance sampling are the
following:
The approximate function must have the same (or greater) support as
the true function, and the method degrades in efficiency as the two
functions diverge.
If the ratio of the true function to the approximate function is to be
used as a probability for rejection of samples, then the true function
must be greater than or equal to the approximate function everywhere!

\paragraph{Slice:}
... When is slice sampling a good idea?  DFM, can you help here?

Slice sampling makes use of the point that (in one dimension) a fair
sampling of $f(\pars)$ is a projection onto the $\pars$ axis of a
uniform sampling of the two-dimensional region under the
two-dimensional plot of $f(\pars)$ \vs\ $\pars$.
The $D$-dimensional generalization is that a fair sampling of
$f(\pars)$ is a projection onto the $\pars$ $D$-dimensional
hypersurface of $\pars$ of a uniform sampling of the $D+1$-dimensional
hypervolume under the hypersurface $f(\pars)$ in a hypothetical plot
of $f(\pars)$ over the whole parameter space.
Thinking in terms of the two-dimensional case (for simplicity), slice
samplers make both ``horizontal'' moves in the $\pars$ direction at
fixed $f(\pars)$ and also ``vertical'' moves in the $f(\pars)$
direction.
Because the sampling in the higher-dimensional space is uniform, it is
easy to make moves to very different parts of parameter space, or
different peaks or modes in the function $f(\pars)$.
In our own research, we use a variant of slice sampling for problems
involving samplings governed by Gaussian Processes.\note{DFM: Cite us
  and Murray here.}

\paragraph{Tempering:}
A huge problem for MCMC samplers---which make local moves based on
local positioning in the parameter space---is that there can be
multiple modes in the function $f(\pars)$, separated by valleys of
zero or near-zero density.
A vanilla MCMC method cannot easily cross such valleys with local
moves.
The idea behind simulated tempering and related methods is to ``smooth
out'' the function $f(\pars)$ or reduce its dynamic range, to make the
peaks less tall and the valleys less deep, to permit the MCMC random
walk to cross the valleys.
The standard method is to introduce a ``temperature'' variable and,
while sampling, take the likelihood function to the power of the
inverse temperature (or, if working in the logarithm, multiply the log
likelihood by the inverse temperature).
When the temperature is high, this reduces the influence of the
likelihood function relative to the prior pdf, and (assuming that the
prior pdf is easy to sample) makes movement in the parameter space
easier.
At each step, when the temperature has moved away from unity, the
distribution being sampled is not exactly $f(\pars)$, but the results
of the sampling are only kept for those samples taken at unit
temperature; these in the end make a fair sampling.

\paragraph{Nested:}
Similar to tempering-like methods are a class of methods called ``nested''
samplers, which also smoothly change the target distribution away from $f(\pars)$.
In the nested case, instead of increasing the temperature, the
samplings are of a censored version of the prior, censored by the
value of the likelihoood function.
When the likelihood censoring is strong, only the most high-likelihood
parts of the prior get sampled; when the likelihood censoring is weak,
almost all of the prior gets sampled.
The idea behind nested sampling---like tempering---is that it is
designed to be a good method for exploring the full parameter space or
searching for all of the modes of the posterior pdf.
It is also designed such that it produces not just a sampling but also
an estimate of the integral $Z$ of the likelihood times the prior
(the Bayes factor or fully marginalized likelihood).

\paragraph{Data augmentation:}
...HOGG?

\paragraph{Reversible-jump:}
Finally, there are some extreme problems in which it is not just hard
to sample in the parameter space, but the parameter space itself is
not of fixed dimension.
For example, if you are modeling an astronomical image with a
collection of stars, and you don't know how many stars to use; in this
case the number of stars itself is a parameter, so the number of
parameters is itself a function of the parameters.\note{HOGG Cite
  Brewer et al}
In these cases you can only use certain kinds of samplers (though M--H
MCMC is one you can choose), and, in addition, you need to make
special kinds of proposals that permit the system to jump from one
parameter space to another that has different dimensionality.
In these cases, the concept of detailed balance becomes non-trivial,
but there are criteria for creating reversible-jump proposals between
the parameter spaces.
These methods are best built and operated under the supervision of a
trained professional.

\section{Likelihoods and priors}

MCMC is used to obtain samples $x_k$ from pdf $p(x)$,
  or a badly normalized all-positive function $f(x)$
  that is different from the pdf by an unknown factor $Z$.
In the context of data analysis,
  MCMC is usually being used to obtain samples
  that are \emph{parameter values} for a probabilistic model of some data.
That is, MCMC is sampling a pdf \emph{for the parameters}.
If it is returning parameter values, then it is not sampling a pdf for the data.
That is, MCMC can only sample a posterior (or prior) pdf for the parameters.
\emph{It cannot sample a likelihood.}%
\note{Well, technically, MCMC \emph{can} be used to sample a likelihood function,
  but the samples would be samples of possible \emph{data} not possible \emph{parameters}.
  The purist might say that even this is not sampling a likelihood function,
  because you should only call it a ``likelihood function'' in contexts in which you are
  treating the data as fixed (at the true data values) and the parameters as variable.
  In this context, the likelihood function is \emph{not} a pdf for the parameters,
  so it can't be sampled.}

Despite all this, in many cases,
  data analysts believe they are sampling the likelihood.
This is because (we presume) they have put a likelihood function
  (or log-likelihood function)
  in as the input to the MCMC code, where the probability function
  (or log-probability function) should go.
Then is it the likelihood that is being sampled?
No, not really;
  it is a posterior probability that is directly proportional to the likelihood function.
That is, it is a posterior probability for some implicit (and improper) ``flat'' priors.

...Give a sense of the difficulties you get into if you fail here; like
discussion with Sven Kreiss on 2013-02-14...

...Explain how your code \emph{should look} with a prior function and a likelihood function...

...It is a good idea to have proper priors.
It is a good test of your priors that you can sample with a ``null'' likelihood function.
You can often have your priors wrong despite having beautiful code.

...You must describe your priors in the same parameterization that you ``step'' in.
That is, if you change your parameters, you \emph{must} change your priors correspondingly.
Give the example of going from $A$, $\phi$ to $A\,\cos\phi$, $A\,\sin\phi$.

\section{Troubleshooting and advice}\label{sec:troubleshooting}

When issues arise with MCMC sampling,
  they are sometimes difficult to diagnose:
Is it the MCMC code, the priors, the likelihood function,
  the initialization, the tuning, or something else?

\paragraph{functional testing:}
In addition to a full set of unit tests for every part of your code
  (yeah \emph{right}\note{If you aren't unit testing, you are probably making some very big mistakes.}),
  we recommend building some end-to-end functional tests that permit you to perform MCMC
  runs with output that must meet certain expectations.
The simplest kind of functional test is to sample a known distribution,
  and check that the sampling has the moments you expect (to within tolerances).
For example, you can sample a $d$-dimensional Gaussian distribution with known non-trivial covariance,
  and check that the empirical covariance comes out as expected.
This is an easy test to fail, so a success builds confidence in any MCMC code.

In the most extreme form of this test,
  for a Gaussian in $d$ dimensions,
  build the covariance matrix input to the Gaussian distribution
  by taking the sum of the outer product of $d+1$ randomly generated vectors.
Then, before sampling, get the eigenvalues and eigenvectors of the matrix.
Set up the MCMC sampler with the Gaussian function with this covariance matrix
  as the probability function (or the log of it as the log-probability function).
Sample for many autocorrelation times,
  or until you are confident you are getting good samples.
Check that the empirical $d$-dimensional covariance of the samples
  has the same (or similar) eigenvectors and eigenvlues to the input matrix.
Also check that the individual one-dimensional samplings
  (along each axis or any projection in the $d$-space)
  look Gaussian and have nearly vanishing skew and kurtosis.
It takes some thinking and some calculating to see ``how well'' these things all ought to match
  expectations, but one thing that can be said with confidence is that agreement should improve
  as the sampling proceeds.
If it doesn't, it is likely that there is something broken about the sampler itself.

Another important functional test is to \emph{sample the prior pdf}.
This tests the sampler,
  tests that your priors really are what you think they are
  (and there are so many ways for them \emph{not} to be\note{HOGG}),
  and tests that your prior pdfs are proper
  (functions that can be normalized).
This test is most effective as a true end-to-end test of your model
  if you structure your code such that the log probability function \code{lnprob}
  input to the MCMC scheme
  is set up as a sum of a log likelihood function \code{lnlikelihood}
  and a prior function \code{lnprior}.
The sampler can be switched to sampling the prior by a simple replacement
  of the \code{lnlikelihood} function
  with a trivial function that always returns zero.

MCMC should run well in this flat-likelihood case (priors are usually easy to sample)
  and the output sample histograms should look like what you expect
  given the prior pdfs you coded.
If the walkers run off to positive or negative infinity,
  and nothing seems to converge,
  your priors are probably not proper.
If the priors don't look as you expect,
  you either have a bug in your code,
  or else a think-o about how your priors ought to look.
Either way, diagnosis in order.

\paragraph{visualization:}
...Make triangle plots.  Great value!...  Show example here.

\paragraph{likelihood issues:}
Sometimes there can be problems with the likelihood function itself.
For example,
  when you re-call the likelihood function with the same parameters,
  do you get \emph{exactly} the same answer?
If you don't, your likelihood function effectively depends on some random numbers;
  maybe it includes within it an integral performed by Monte Carlo method?
In general, you want to change your likelihood function such that,
  even if it includes a numerical or stochastic integral,
  it is designed such that it produces \emph{precisely} the same
  value when it returns to the same point in parameter space.
One way to make this happen is to choose the random numbers
  (those used for any internal integration inside the likelihood function)
  in advance and re-use them on every likelihood call.
This provides the same integral according to the same approximation,
  but both speeds up the code (no re-generation of random numbers)
  and also makes the likelihood function single valued.
Even better is to replace internal Monte Carlo integrals with
  deterministic numerical (or even better analytic) integrals.

If things are giving trouble and you suspect the likelihood function,
  another important test is to visualize slices through parameter space.
When you call the likelihood function on a grid in each parameter
  (with the others fixed, say, to reasonable values),
  you should see a likelihood value that is a smooth function of each parameter,
  or at least as smooth as you expect.
Things like numerical integration,
  truncated expansions,
  or adaptive approximations
  inside likelihood functions
  can make the function noisy or jagged at small scales in parameter space.
These problems hurt optimizers more than samplers,
  but they usually point to code issues.
In general, plotting the likelihood function as a function of parameter along
  slices in parameter space often reveals bugs or think-os.

SHOW EXAMPLES?

\paragraph{low acceptance fraction:}
If you find that you are getting low acceptance fraction
  (in standard M-H MCMC or other varieties for which there is such a concept)
  you can simply reduce the sizes of the steps you consider to increase the fraction that are accepted.
This means decreasing the variance of the proposal distribution,
  or whatever parameter or parameters control the width of that distribution.
If reducing the variance of the proposal distribution does \emph{not} increase the acceptance fraction,
  then \emph{you have a bug}.
You must find that bug by auditing your code or else returning to the functional tests listed above.

Similarly for high acceptance fraction:
If you are finding a high acceptance fraction,
  then increasing the variance of the proposal distributon \emph{must} decrease the acceptance fraction.
If it does \emph{not}, then \emph{you have a bug}.

\paragraph{sticky chains:}
If you are concerned about acceptance fraction or convergence,
  it makes sense to plot parameter values as a function of ``step number'' or iteration number.
That is, plot the ordered chain in each parameter dimension.
These plots will have long horizontal patches if the chain is getting stuck;
  a converged chain will show the walker traversing the parameter space in every dimension fully many times
  over the length of the run.
Not a few times; many times.
If you are using an ensemble sampler\note{CITE EMCEE},
  make plots showing all $M$ walkers, each a different color.
A converged run for an ensemble sampler will show every single walker traversing the parameter space
  in every dimension many times.

SHOW EXAMPLES OF GOOD AND BAD?

\paragraph{initialization-dependence:}
If you initialize your MCMC runs in different places in parameter space,
  do you get the same (or very similar) final parameter samplings,
  in mean and variance?
If you don't,
  then your posterior PDF is badly multi-modal;
  trivial MCMC methods probably won't fix your problem.
Not to put too fine a point on it:
If your results are initialization-dependent,
  then your MCMC runs \emph{are not converged}.

You either have to go to a method that tries to explore the posterior
  more liberally, such as nested sampling or simulated tempering,
  or else split your ``model'' up into several sub-models,
  each of which contains different posterior modes.
In general, this problem is hard to fix;
  as we mentioned above, it is provably impossible
  to explore all of parameter space if the posterior PDF is
  complex in morphology.
We don't have very useful advice here, except to
  spend time learning about the multiple modes in the posterior PDF
  and what each ``means'' or corresponds to,
  and to do your best to express in your results the multiple optima.
One of the miracles of MCMC is that if you \emph{can} fairly sample
  as complex posterior PDF,
  the fraction of samples in each mode
  tends to the relative total integrated posterior probability inside each mode,
  as the sampling converges.

Ensemble samplers react to multiple modes
  by obtaining very low acceptance ratio,
  because the samplers in the different modes are not easily able to ``help one another''
  move efficiently.
Again, if the acceptance ratio is low and there appear to be multiple modes,
  it is likely that the sampling will also be initialization dependent,
  and it is very unlikely that your sampling is going to be converged.

\paragraph{parameterization problems:}
It helps to think carefully about how to parameterize.
For example, if there is an angle $\phi$ in your problem,
  and your prior requires it to be between $0$ and $2\pi$,
  and the posterior mode is very near $0$,
  a little bit of probability leaking below $0$ plus the prior cutoffs plus mod-$2\pi$ symmetry
  makes an intrinsically unimodal problem multimodal!
In these cases, we usually advise reparameterization from a (say) amplitude $A$ and angle $\phi$
  to two vector components $a\equiv A\,\cos\phi$ and $b\equiv A\,\sin\phi$.
Of course this re-parameterization changes significantly the \emph{form} of the prior PDF
  (the transformation brings in a Jacobian);
  it has to be adjusted with care.

DFM: SHOULD WE DO THIS ALL EXPLICITLY HERE IN MATH?
Choose a $p(A)$ and a $p(\phi) = [2\pi]^{-1}$,
   and then transform to $p(a, b)$.

%% ...Priors: Once again:
%% If you describe your priors in one parameterization but
%% then perform sampling in a different space, it is easy to make
%% mistakes.  For example: Flat prior in amplitude and phase, sample in
%% $A\,\cos\phi$ and $A\,\sin\phi$.  There are many ways to do this
%% wrong.

For another example, sometimes there is almost an exact degeneracy between two parameters,
  say $a$ and $b$.  Then it makes sense to switch to parameters $a+b$ and $a-b$,
  or some other linear combinations where correlations or near-degeneracies are likely to be reduced.
Again, such transformations require also prior transformations, which must be made with care.
There are some affine invariant samplers\note{CITE EMCEE AGAIN HERE} that are invariant to such transformations,
  but if you aren't using such a sampler,
  it is worth getting out ahead of these problems.
They can be found by performing an exploratory sampling,
  making a triangle plot as described above,
  and looking for narrow diagonal lines in the two-dimentional scatter plots.

HOGG:  How to deal with discrete or integer or binary parameters?

\paragraph{model checking:}
...Beyond the scope of this document...
Something about cross-validation and posterior predictive checks...
Something about plotting models (drawn from posterior) on data, where appropriate...
How could you possibly know that your sampling really represents your \emph{real} posterior PDF?..

...The Bayes integral / evidence integral?

...Posterior predictive checks?

%% HOGG DELETED / COMMENTED OUT THE BELOW
%% - make sure everything below is discussed somewhere above and then delete it below.

%% ...breaking Markov-ness.
%% % ***** BJB
%% % "Diminishing adaptations" is an idea that is quite easy
%% % to explain. There are other ways to break Markov-ness but
%% % this probably covers enough of them for an intro document
%% % ***** Hogg
%% % I was going to say you should NEVER break Markov-ness
%% % unless you REALLY understand what you are doing.

%% ...Posteriors:  You can do anything you like (within
%% reason) to the output samples and be sampling the posterior in that
%% transformed quantity.

%% ...How to deal with forbidden regions in prior, or sharp prior boundaries?

%% ...How to deal with constraints that pin you to a subspace of the parameter space?  Method-dependent.

%% ...Initialization and burn-in advice we haven't addressed earlier?

%% ...Finally, to repeat: You can't sample the likelihood!  You can only
%% sample a posterior pdf!

%% % \begin{figure}[htbp]
%% % \exampleplot{ex17}
%% % \caption{Partial solution to \problemname~\ref{prob:bayesintrinsic}:
%% % The marginalized posterior probability distribution for the intrinsic
%% % variance.}\label{fig:bayesintrinsic}
%% % \end{figure}

\clearpage
\markright{Notes}\theendnotes

\clearpage
\begin{thebibliography}{}\markright{References}
\bibitem[Foreman-Mackey et al.(2012)]{emcee}
  Foreman-Mackey,~D., Hogg,~D.~W., Lang,~D., \& Goodman,~J.\ 2012,
  \project{emcee}: The MCMC Hammer,
  \arxiv{1202.3665}
\end{thebibliography}

\end{document}
